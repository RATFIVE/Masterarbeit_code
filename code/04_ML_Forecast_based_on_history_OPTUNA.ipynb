{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856e6cff",
   "metadata": {},
   "source": [
    "## Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5543ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating pressure_msl: 100%|██████████| 20184/20184 [00:17<00:00, 1146.65it/s]\n",
      "Interpolating surface_pressure: 100%|██████████| 20184/20184 [00:04<00:00, 4944.00it/s]\n",
      "Interpolating wind_gusts_10m: 100%|██████████| 20184/20184 [00:03<00:00, 5142.02it/s]\n",
      "Interpolating wind_u: 100%|██████████| 20184/20184 [00:03<00:00, 5327.08it/s]\n",
      "Interpolating wind_v: 100%|██████████| 20184/20184 [00:04<00:00, 4914.09it/s]\n",
      "100%|██████████| 20161/20161 [00:02<00:00, 9351.95it/s]\n",
      "100%|██████████| 20161/20161 [00:02<00:00, 9859.40it/s] \n",
      "100%|██████████| 20161/20161 [00:02<00:00, 9133.35it/s]\n",
      "100%|██████████| 20161/20161 [00:02<00:00, 9750.01it/s] \n",
      "100%|██████████| 20161/20161 [00:02<00:00, 9201.82it/s]\n",
      "100%|██████████| 20161/20161 [00:02<00:00, 9273.01it/s] \n",
      "100%|██████████| 20161/20161 [00:02<00:00, 9933.11it/s] \n",
      "100%|██████████| 20161/20161 [00:02<00:00, 9077.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time  bottomT_PC_1  sla_PC_1  sla_PC_2   so_PC_1   so_PC_2  \\\n",
      "0 2022-12-09 22:00:00      0.634133  0.925309 -0.457366 -0.100705 -0.897528   \n",
      "1 2022-12-09 23:00:00      0.637294  0.940802 -0.442214 -0.111268 -0.912123   \n",
      "2 2022-12-10 00:00:00      0.639464  0.945689 -0.414826 -0.122708 -0.920838   \n",
      "3 2022-12-10 01:00:00      0.638936  0.965772 -0.330261 -0.133696 -0.925385   \n",
      "4 2022-12-10 02:00:00      0.641412  0.952780 -0.323189 -0.143205 -0.932978   \n",
      "\n",
      "    so_PC_3  sob_PC_1  sob_PC_2  thetao_PC_1   uo_PC_1   uo_PC_2   uo_PC_3  \\\n",
      "0  2.029672  0.280786 -0.082470    -0.553418  0.653585 -0.046777  0.145592   \n",
      "1  2.036069  0.293755 -0.104575    -0.554901  0.482343 -0.024300  0.008832   \n",
      "2  2.038312  0.307333 -0.125272    -0.556919  0.236408 -0.041806 -0.088139   \n",
      "3  2.038564  0.321025 -0.144447    -0.555702  0.027601 -0.015676 -0.083613   \n",
      "4  2.051886  0.334548 -0.161840    -0.557908 -0.064479  0.086782 -0.048168   \n",
      "\n",
      "    uo_PC_4   uo_PC_5   vo_PC_1   vo_PC_2   vo_PC_3   vo_PC_4   vo_PC_5  \\\n",
      "0  0.725800  0.540069  0.758238 -0.662183  0.058765  0.961353 -1.370138   \n",
      "1  0.675426  0.501529  0.877569 -0.560790 -0.166088  0.703747 -1.363841   \n",
      "2  0.384520  0.432956  0.935413 -0.575423 -0.322491  0.430572 -1.377934   \n",
      "3 -0.008058  0.182592  0.809479 -0.619018 -0.158872  0.508551 -1.490852   \n",
      "4 -0.185050 -0.066492  0.650262 -0.656673  0.038886  0.656122 -1.544152   \n",
      "\n",
      "    vo_PC_6   wo_PC_1  pressure_msl_PC_1  surface_pressure_PC_1  \\\n",
      "0  0.596323  0.210841          -0.330321              -0.354132   \n",
      "1  0.811406  0.250500          -0.321885              -0.345880   \n",
      "2  1.024291  0.291685          -0.317504              -0.341250   \n",
      "3  1.226882  0.046254          -0.297662              -0.321753   \n",
      "4  1.264545 -0.057197          -0.300356              -0.324705   \n",
      "\n",
      "   wind_gusts_10m_PC_1  wind_gusts_10m_PC_2  wind_gusts_10m_PC_3  \\\n",
      "0            -1.072497            -0.911858             0.096909   \n",
      "1            -1.143070            -0.966598             0.118110   \n",
      "2            -1.159793            -0.983756             0.197115   \n",
      "3            -1.218121            -0.743925             0.050638   \n",
      "4            -1.211565            -0.937795             0.063872   \n",
      "\n",
      "   wind_gusts_10m_PC_4  wind_u_PC_1  wind_u_PC_2  wind_v_PC_1  wind_v_PC_2  \\\n",
      "0            -0.456315     0.058208    -0.554387    -0.044919     0.730433   \n",
      "1            -0.305792     0.103238    -0.617209    -0.109434     0.772025   \n",
      "2            -0.201875     0.155376    -0.417863    -0.161271     1.380628   \n",
      "3            -0.398744     0.169954    -0.559172    -0.238492     1.189508   \n",
      "4            -0.086306     0.172763    -0.732738    -0.312825     1.104764   \n",
      "\n",
      "   wind_v_PC_3  \n",
      "0     0.945476  \n",
      "1     0.848582  \n",
      "2     0.419826  \n",
      "3     0.512767  \n",
      "4     0.593599  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## Import Libaries\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, recall_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from utils.dl_helper_functions import (\n",
    "    create_sequences,\n",
    "    load_picture_lagged_data,\n",
    "    scale_data,\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "HORIZON = 24 * 3 # 3 days of forecast\n",
    "INITIAL_TRAINING_SIZE = 24 * 183   # 6 months of data = 4392 h\n",
    "SEQUENCE_LENGTH = 24  # 1 day of data\n",
    "DTYPE_NUMPY = np.float32  # Use float32 for numpy arrays\n",
    "n_jobs = -1  # Use all available CPU cores for parallel processing\n",
    "\n",
    "\n",
    "# # Load Data\n",
    "X, y_lagged, y, common_time = load_picture_lagged_data(return_common_time=True, verbose=False, grid_size=5, n_jobs=n_jobs, dtype=DTYPE_NUMPY, pca=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbfe3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 16:37:29,589] A new study created in RDB with name: Linear\n",
      "[I 2025-07-05 16:39:11,259] Trial 0 finished with value: 0.2395572236324822 and parameters: {}. Best is trial 0 with value: 0.2395572236324822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished trial with model Linear and params {}, score: 0.2395572236324822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-05 16:39:13,018] Trial 1 finished with value: 0.2395572236324822 and parameters: {}. Best is trial 0 with value: 0.2395572236324822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished trial with model Linear and params {}, score: 0.2395572236324822\n",
      "Beste Parameter: {}\n",
      "Bester Score: 0.2395572236324822\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Annahmen (vorab gesetzt)\n",
    "DTYPE_NUMPY = np.float32\n",
    "n_jobs = -1\n",
    "\n",
    "# Daten vorbereiten\n",
    "X = X.astype(DTYPE_NUMPY)\n",
    "y_lagged = y_lagged.astype(DTYPE_NUMPY)\n",
    "y = y.astype(DTYPE_NUMPY)\n",
    "\n",
    "# Cross-Validation Zeitpunkte\n",
    "folds = {\n",
    "    \"Surge1\": pd.Timestamp(\"2023-02-25 16:00:00\"),\n",
    "    \"Surge2\": pd.Timestamp(\"2023-04-01 09:00:00\"),\n",
    "    \"Surge3\": pd.Timestamp(\"2023-10-07 20:00:00\"),\n",
    "    \"Surge4\": pd.Timestamp(\"2023-10-20 21:00:00\"),\n",
    "    \"Surge5\": pd.Timestamp(\"2024-01-03 01:00:00\"),\n",
    "    \"Surge6\": pd.Timestamp(\"2024-02-09 15:00:00\"),\n",
    "    \"Surge7\": pd.Timestamp(\"2024-12-09 10:00:00\"),\n",
    "    \"normal1\": pd.Timestamp(\"2023-07-01 14:00:00\"),\n",
    "    \"normal2\": pd.Timestamp(\"2024-04-01 18:00:00\"),\n",
    "    \"normal3\": pd.Timestamp(\"2025-01-01 12:00:00\"),\n",
    "}\n",
    "\n",
    "\n",
    "def custom_score(y_true=None, y_pred=None, bins=[1, 2.00], alpha=0.7):\n",
    "    \n",
    "    # Initialisiere Recall- und MSE-Werte\n",
    "    recalls = []\n",
    "    for i in range(y_true.shape[1]):  # Iteriere über jede Spalte\n",
    "        y_true_class = np.digitize(y_true[:, i], bins=bins)\n",
    "        y_pred_class = np.digitize(y_pred[:, i], bins=bins)\n",
    "        recalls.append(recall_score(y_true_class, y_pred_class, average=\"macro\"))\n",
    "    \n",
    "    mean_recall = np.mean(recalls)  # Durchschnittlicher Recall\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return alpha * (1 - mean_recall) + (1 - alpha) * mse\n",
    "\n",
    "\n",
    "def get_model(name, trial_params=None):\n",
    "    if trial_params is None:\n",
    "        trial_params = {}\n",
    "\n",
    "    if name == \"RandomForest\":\n",
    "        return MultiOutputRegressor(RandomForestRegressor(random_state=42, n_jobs=n_jobs, **trial_params), n_jobs=n_jobs)\n",
    "    elif name == \"SVR\":\n",
    "        return MultiOutputRegressor(SVR(**trial_params), n_jobs=n_jobs)\n",
    "    elif name == \"XGBoost\":\n",
    "        return MultiOutputRegressor(XGBRegressor(random_state=42, n_jobs=n_jobs, **trial_params), n_jobs=n_jobs)\n",
    "    elif name == \"LGBM\":\n",
    "        return MultiOutputRegressor(LGBMRegressor(random_state=42, n_jobs=n_jobs, **trial_params), n_jobs=n_jobs)\n",
    "    elif name == \"Linear\":\n",
    "        return MultiOutputRegressor(LinearRegression(n_jobs=n_jobs), n_jobs=n_jobs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unbekanntes Modell: {name}\")\n",
    "\n",
    "\n",
    "def cross_validation_loop(model_name, folds, X, y_lagged, y, common_time, time_delta, trial_params=None):\n",
    "    fold_results = []\n",
    "\n",
    "    for surge_name, fold in folds.items():\n",
    "        start_cutoff = fold - time_delta\n",
    "        end_cutoff = fold + time_delta\n",
    "        idx_start_cutoff = np.where(common_time == start_cutoff)[0][0]\n",
    "        idx_end_cutoff = np.where(common_time == end_cutoff)[0][0]\n",
    "\n",
    "        X_test = X[idx_start_cutoff:idx_end_cutoff]\n",
    "        y_lagged_test = y_lagged[idx_start_cutoff:idx_end_cutoff]\n",
    "        y_test = y[idx_start_cutoff:idx_end_cutoff]\n",
    "\n",
    "        X_train = X.copy()\n",
    "        y_lagged_train = y_lagged.copy()\n",
    "        y_train = y.copy()\n",
    "\n",
    "        X_train[idx_start_cutoff:idx_end_cutoff] = np.nan\n",
    "        y_lagged_train[idx_start_cutoff:idx_end_cutoff] = np.nan\n",
    "        y_train[idx_start_cutoff:idx_end_cutoff] = np.nan\n",
    "\n",
    "        X_train, y_lagged_train, y_train = create_sequences(X_train, y_lagged_train, y_train, SEQUENCE_LENGTH, 24)\n",
    "        X_test, y_lagged_test, y_test = create_sequences(X_test, y_lagged_test, y_test, SEQUENCE_LENGTH, 24)\n",
    "\n",
    "        gap = 168\n",
    "        X_test = X_test[gap:-gap]\n",
    "        y_lagged_test = y_lagged_test[gap:-gap]\n",
    "        y_test = y_test[gap:-gap]\n",
    "\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "\n",
    "        \n",
    "        data = scale_data(scaler_X, scaler_y,\n",
    "                          X_train, y_lagged_train, y_train,\n",
    "                          None, None, None,\n",
    "                          X_test, y_lagged_test, y_test,\n",
    "                          dtype=DTYPE_NUMPY, verbose=False)\n",
    "\n",
    "        X_train, y_lagged_train, y_train, _, _, _, X_test, y_lagged_test, y_test = data\n",
    "\n",
    "        X_train = np.hstack([X_train.reshape(X_train.shape[0], -1), y_lagged_train.reshape(y_lagged_train.shape[0], -1)])\n",
    "        X_test = np.hstack([X_test.reshape(X_test.shape[0], -1), y_lagged_test.reshape(y_lagged_test.shape[0], -1)])\n",
    "\n",
    "        model = get_model(model_name, trial_params=trial_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        score = custom_score(y_test, y_pred)\n",
    "        fold_results.append(score)\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    \n",
    "\n",
    "    if model_name == \"RandomForest\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 100),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 50),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 100),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"]),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]), \n",
    "        }\n",
    "    elif model_name == \"SVR\":\n",
    "        params = {\n",
    "            \"C\": trial.suggest_loguniform(\"C\", 0.01, 50, log=True),\n",
    "            \"epsilon\": trial.suggest_loguniform(\"epsilon\", 0.01, 1.0, log=True),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"rbf\", \"linear\", \"poly\"]),\n",
    "            \"degree\": trial.suggest_int(\"degree\", 1, 2) if trial.suggest_categorical(\"kernel\", [\"rbf\", \"linear\", \"poly\"]) == \"poly\" else None,\n",
    "            \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]) if trial.suggest_categorical(\"kernel\", [\"rbf\", \"linear\", \"poly\"]) == \"rbf\" else None,\n",
    "        }\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 10.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 10.0),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "            \"verbosity\": 0\n",
    "        }\n",
    "    elif model_name == \"LGBM\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 100),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.9, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 200),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.0, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 15.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 15.0),\n",
    "            \"verbosity\": -1\n",
    "        }\n",
    "    else:\n",
    "        params = {}\n",
    "\n",
    "    scores = cross_validation_loop(model_name, folds, X, y_lagged, y, common_time, pd.Timedelta(hours=168 * 4), params)\n",
    "    score = np.mean(scores)\n",
    "    print(f\"Finished trial with model {model_name} and params {params}, score: {score}\")\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"Linear\"  # z.B. \"SVR\", \"XGBoost\", etc.\n",
    "storage = f\"sqlite:///Versuch3_{model_name}.db\"  # SQLite-Datenbank für Optuna\n",
    "n_trials = 2  # Anzahl der Versuche für die Hyperparameter-Optimierung\n",
    "# Optuna Study starten\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=f\"{model_name}\", storage=storage, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "print(\"Beste Parameter:\", study.best_params)\n",
    "print(\"Bester Score:\", study.best_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geomar-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
