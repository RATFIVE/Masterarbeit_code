{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fragen\n",
        "\n",
        "1. Analyse Allgemein\n",
        "    - Lässt sich ein Zusammenhang zwischen dem Wasserpegel von den Beobachtungsdaten und den Modelldaten erkennen?\n",
        "    - Wie ist die Korrelation zwischen Wasserpegel Model und Wasserpegel Beobachtung\n",
        "\n",
        "2. Analyse Sturmfluten\n",
        "    - Wie verhält sich Wind, Windrichtung bei den unterschiedlichen Sturmfluten\n",
        "    - Wie vehält sich Wassergeschwindigkeit, Richtung bei den unterschiedlichen Sturmfluten\n",
        "    - Lassen sich Korrelationen zwischen den Features und dem Wasserpegel (sla) erkennen? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpOApovX9ZhU"
      },
      "source": [
        "## Import Libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E2fH33dHzxc"
      },
      "outputs": [],
      "source": [
        "# import all necessary libraries\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import cartopy.feature as cfeature\n",
        "import geodatasets\n",
        "import geopandas as gpd\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shapely.geometry\n",
        "import xarray as xr\n",
        "from joblib import Parallel, delayed\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "from scipy.interpolate import griddata\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from tqdm import tqdm\n",
        "from utils.eda_helper_functions import plot_histogram\n",
        "\n",
        "from utils.eda_helper_functions import (\n",
        "    check_missing_times,\n",
        "    group_data_hourly,\n",
        "    load_insitu_data,\n",
        "    load_ocean_data,\n",
        "    load_weather_data,\n",
        "    plot_water_level_anomalies,\n",
        "    process_df,\n",
        "    process_flensburg_data,\n",
        "    show_df,\n",
        ")\n",
        "from utils.config import (\n",
        "    LAT_FLENSBURG,\n",
        "    LON_FLENSBURG,\n",
        "    SUB_BOX,\n",
        "    OCEAN_DICT,\n",
        "    WEATHER_DICT,\n",
        "    INSITU_DICT,\n",
        "    OCEAN_POINTS,\n",
        "    WEATHER_POINTS,\n",
        "    )\n",
        "\n",
        "# Ignore SettingWithCopyWarning:\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
        "\n",
        "# Display all columns\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.size\": 18,                # Grundschriftgröße (wirkt auf alles, sofern nicht überschrieben)\n",
        "    \"axes.titlesize\": 18,           # Größe des Titels der Achse (z.B. 'Subplot Title')\n",
        "    \"axes.labelsize\": 18,           # Achsenbeschriftung (x/y label)\n",
        "    \"xtick.labelsize\": 18,          # X-Tick-Beschriftung\n",
        "    \"ytick.labelsize\": 18,          # Y-Tick-Beschriftung\n",
        "    \"legend.fontsize\": 18,          # Legendentext\n",
        "    \"figure.titlesize\": 18,         # Gesamttitel der Abbildung (plt.suptitle)\n",
        "    \"figure.labelsize\": 18,         # (optional, selten verwendet)\n",
        "    \"savefig.dpi\": 300,             # DPI beim Speichern\n",
        "    \"figure.dpi\": 72,              # DPI bei Anzeige\n",
        "})\n",
        "\n",
        "\n",
        "ocean_data_path = Path(f\"../data/numerical_data/points{OCEAN_POINTS}\")\n",
        "print(ocean_data_path)\n",
        "weather_data_path = Path(f\"../data/numerical_data/points{WEATHER_POINTS}\")\n",
        "print(weather_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_feature_distribution(df:pd.DataFrame, features:list, bins:int=50, save_png:bool=False, one_plot:bool=True, prefix:str=\"\") -> None:\n",
        "    \"\"\"\n",
        "    Plots the distribution of features in a DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the features.\n",
        "        features (list): List of feature names to plot.\n",
        "        bins (int): Number of bins for the histogram.\n",
        "        save_png (bool): Whether to save the plot as a PNG file.\n",
        "    \"\"\"\n",
        "    if one_plot:\n",
        "        n_cols = 2\n",
        "        n_rows = (len(features) + n_cols - 1) // n_cols\n",
        "\n",
        "        fig = plt.figure(figsize=(8 * n_cols, 5 * n_rows))\n",
        "        gs = gridspec.GridSpec(n_rows, n_cols, figure=fig)\n",
        "\n",
        "        fig.suptitle(\"Feature Distribution\", y=0.98)\n",
        "\n",
        "        for idx, feature in tqdm(enumerate(features), total=len(features), desc=\"Plotting features\"):\n",
        "            #print(f\"Plotting distribution for {feature}\")\n",
        "            ax = fig.add_subplot(gs[idx])\n",
        "            plot_histogram(df, column=feature, bins=bins, ax=ax, show_stats=True)\n",
        "\n",
        "        # Statt tight_layout --> subplots_adjust\n",
        "        fig.subplots_adjust(top=0.92, hspace=0.5, wspace=0.3)  # <-- manuell fein justieren!\n",
        "        if save_png:\n",
        "            plt.savefig(f\"../figures/distribution_{feature}.png\", dpi=300, bbox_inches='tight')\n",
        "    else:\n",
        "        from matplotlib import rc_context\n",
        "\n",
        "        with rc_context({\n",
        "            \"font.size\": 20,                # Grundschriftgröße (wirkt auf alles, sofern nicht überschrieben)\n",
        "            \"axes.titlesize\": 20,           # Größe des Titels der Achse (z.B. 'Subplot Title')\n",
        "            \"axes.labelsize\": 20,           # Achsenbeschriftung (x/y label)\n",
        "            \"xtick.labelsize\": 20,          # X-Tick-Beschriftung\n",
        "            \"ytick.labelsize\": 20,          # Y-Tick-Beschriftung\n",
        "            \"legend.fontsize\": 20,          # Legendentext\n",
        "            \"figure.titlesize\": 20,         # Gesamttitel der Abbildung (plt.suptitle)\n",
        "            \"figure.labelsize\": 20,         # (optional, selten verwendet)\n",
        "            \"savefig.dpi\": 300,             # DPI beim Speichern\n",
        "            \"figure.dpi\": 72,              # DPI bei Anzeige\n",
        "        }):\n",
        "\n",
        "            for feature in tqdm(features, desc=\"Plotting features\"):\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "                plot_histogram(df, column=feature, bins=bins, ax=ax, show_stats=True, title=\" \")\n",
        "                #ax.set_title(f\"Distribution of {feature}\")\n",
        "                if save_png:\n",
        "                    plt.savefig(f\"../thesis_plots/{prefix}distribution_{feature}.png\", dpi=300, bbox_inches='tight')\n",
        "                \n",
        "\n",
        "    plt.show()\n",
        "    plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load The Data & IDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ocean Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Feature | Unit | Description | Explanation |\n",
        "|:---|:---|:---|:---|\n",
        "| bottomT | °C | Sea water potential temperature at sea floor | Temperature of seawater at the ocean floor, accounting for pressure effects. |\n",
        "| mlotst | m | Ocean mixed layer thickness defined by sigma theta | Depth of the ocean's surface layer where temperature and salinity are relatively uniform. |\n",
        "| siconc | - | Sea ice area fraction | Fractional coverage of sea ice in a given area (0 = no ice, 1 = full coverage). |\n",
        "| sithick | m | Sea ice thickness | Thickness of sea ice from surface to bottom. |\n",
        "| sla | m | Sea surface height above sea level | Deviation of the ocean surface from the mean sea level, can indicate currents or tides. |\n",
        "| so | $1 / 10^3$ | Sea water salinity | Salinity of seawater (measured dimensionless, typically expressed in parts per thousand or PSU). |\n",
        "| sob | $1 / 10^3$| Sea water salinity at sea floor | Salinity of seawater at the ocean floor, normalized (0.001 units). |\n",
        "| thetao | °C | Sea water potential temperature | Potential temperature of seawater, referenced to sea surface pressure. |\n",
        "| uo | m/s | Eastward sea water velocity | Velocity component of seawater flow towards the east. |\n",
        "| vo | m/s | Northward sea water velocity | Velocity component of seawater flow towards the north. |\n",
        "| wo | m/s | Upward sea water velocity | Vertical velocity of seawater, positive upward. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ocean = load_ocean_data(ocean_data_path, OCEAN_POINTS, verbose=True)\n",
        "df_ocean.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_distribution(df_ocean, df_ocean.columns, bins=50, save_png=True, one_plot=False, prefix=\"ocean_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ocean = process_df(df_ocean, drop_cols=[\"depth\"], verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weather Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Feature | Unit | Description | Explanation |\n",
        "|:---|:---|:---|:---|\n",
        "| temperature_2m | °C | Temperature (2 m) | Air temperature at 2 meters above ground. |\n",
        "| relative_humidity_2m | % | Relative Humidity (2 m) | Percentage of humidity at 2 meters height. |\n",
        "| dew_point_2m | °C | Dewpoint (2 m) | Temperature at which air moisture condenses (dew point) at 2 meters height. |\n",
        "| apparent_temperature | °C | Apparent Temperature | Perceived temperature considering wind and humidity. |\n",
        "| precipitation_probability | % | Precipitation Probability | Probability of precipitation. |\n",
        "| precipitation | mm | Precipitation (rain + showers + snow) | Total precipitation amount (rain, showers, snow). |\n",
        "| rain | mm | Rain | Precipitation amount due to rain. |\n",
        "| showers | mm | Showers | Precipitation amount due to showers. |\n",
        "| snowfall | cm | Snowfall | Precipitation amount due to snow. |\n",
        "| snow_depth | cm | Snow Depth | Total snow depth on the ground. |\n",
        "| weather_code | - | Weather code | Classification of weather conditions by a code (e.g., sunny, cloudy). |\n",
        "| pressure_msl | hPa | Sealevel Pressure | Atmospheric pressure reduced to sea level. |\n",
        "| surface_pressure | hPa | Surface Pressure | Actual atmospheric pressure at the surface. |\n",
        "| cloud_cover | % | Cloud cover Total | Total cloud coverage. |\n",
        "| cloud_cover_low | % | Cloud cover Low | Cloud coverage by low-level clouds. |\n",
        "| cloud_cover_mid | % | Cloud cover Mid | Cloud coverage by mid-level clouds. |\n",
        "| cloud_cover_high | % | Cloud cover High | Cloud coverage by high-level clouds. |\n",
        "| visibility | m | Visibility | Visibility distance. |\n",
        "| evapotranspiration | mm | Evapotranspiration | Water loss through evaporation and plant transpiration. |\n",
        "| et0_fao_evapotranspiration | mm | Reference Evapotranspiration (ET₀) | Standardized reference evapotranspiration according to FAO. |\n",
        "| vapour_pressure_deficit | hPa | Vapour Pressure Deficit | Difference between saturation and actual vapor pressure. |\n",
        "| wind_speed_10m | km/h | Wind Speed (10 m) | Wind speed at 10 meters above ground. |\n",
        "| wind_speed_80m | km/h | Wind Speed (80 m) | Wind speed at 80 meters above ground. |\n",
        "| wind_speed_120m | km/h | Wind Speed (120 m) | Wind speed at 120 meters above ground. |\n",
        "| wind_speed_180m | km/h | Wind Speed (180 m) | Wind speed at 180 meters above ground. |\n",
        "| wind_direction_10m | ° | Wind Direction (10 m) | Wind direction in degrees at 10 meters height (0° = North). |\n",
        "| wind_direction_80m | ° | Wind Direction (80 m) | Wind direction in degrees at 80 meters height. |\n",
        "| wind_direction_120m | ° | Wind Direction (120 m) | Wind direction in degrees at 120 meters height. |\n",
        "| wind_direction_180m | ° | Wind Direction (180 m) | Wind direction in degrees at 180 meters height. |\n",
        "| wind_gusts_10m | km/h | Wind Gusts (10 m) | Maximum gust wind speed at 10 meters height. |\n",
        "| temperature_80m | °C | Temperature (80 m) | Air temperature at 80 meters above ground. |\n",
        "| temperature_120m | °C | Temperature (120 m) | Air temperature at 120 meters above ground. |\n",
        "| temperature_180m | °C | Temperature (180 m) | Air temperature at 180 meters above ground. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_weather = load_weather_data(weather_data_path, WEATHER_POINTS, verbose=True)\n",
        "plot_feature_distribution(df_weather, df_weather.columns, bins=50, save_png=True, one_plot=False, prefix=\"weather_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_weather = process_df(df_weather, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In Situ Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Feature   | Unit    | Description                     | Explanation                                                                 |\n",
        "|:----------|:--------|:---------------------------------|:---------------------------------------------------------------------------|\n",
        "| time      | -       | Timestamp                        | Date and time of the observation (UTC).                                    |\n",
        "| depth     | m       | Measurement depth                | Depth below sea surface where the measurement was taken.                   |\n",
        "| time_qc   | -       | Time quality control flag        | Quality control indicator for the timestamp (e.g., 1 = good).              |\n",
        "| deph      | m       | Nominal depth                    | Nominal (intended) depth of the measurement, could differ from actual depth.|\n",
        "| latitude  | degrees | Latitude                         | Geographic coordinate specifying north-south position.                     |\n",
        "| longitude | degrees | Longitude                        | Geographic coordinate specifying east-west position.                       |\n",
        "| slev      | m       | Sea level                        | Measured sea surface height relative to a reference Datum |\n",
        "| slev_qc   | -       | Sea level quality control flag   | Quality control indicator for sea level measurement (e.g., 1 = good).       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.eda_helper_functions import load_insitu_data\n",
        "\n",
        "df_insitu = load_insitu_data(verbose=True)\n",
        "plot_feature_distribution(df_insitu, df_insitu.columns, bins=50, save_png=True, one_plot=False, prefix=\"insitu_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.eda_helper_functions import process_flensburg_data\n",
        "\n",
        "df_insitu = process_flensburg_data(df_insitu, \n",
        "                                      start_time=df_ocean['time'].min(),\n",
        "                                      end_time=df_ocean['time'].max(),\n",
        "                                      verbose=True, order=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.eda_helper_functions import group_data_hourly\n",
        "\n",
        "df_insitu = group_data_hourly(df_insitu)\n",
        "df_insitu = process_df(df_insitu, drop_cols=[\"deph\"], verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_insitu.info())\n",
        "print(df_insitu.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Flensburg Observation Waterlevel Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.eda_helper_functions import plot_water_level_anomalies\n",
        "\n",
        "fig, ax = plot_water_level_anomalies(df_insitu)\n",
        "plt.show()\n",
        "\n",
        "import datetime\n",
        "\n",
        "sturm_surge_list = [datetime.datetime(2023, 2, 25, 17, 0),\n",
        "                    datetime.datetime(2023, 4, 1, 12, 0),\n",
        "                    datetime.datetime(2023, 10, 7, 20, 0),\n",
        "                    datetime.datetime(2023, 10, 20, 0, 0),\n",
        "                    datetime.datetime(2024, 1, 3, 9, 0),\n",
        "                    datetime.datetime(2024, 2, 9, 18, 0),\n",
        "                    datetime.datetime(2024, 12, 9, 16, 0),\n",
        "                    ]\n",
        "\n",
        "for time in sturm_surge_list:\n",
        "    start_time = time - datetime.timedelta(days=3)\n",
        "    end_time = time + datetime.timedelta(days=3)\n",
        "    df_insitu_sturm = df_insitu[(df_insitu[\"time\"] >= start_time) & (df_insitu[\"time\"] <= end_time)]\n",
        "    plot_water_level_anomalies(df_insitu_sturm, start_date=start_time, end_date=end_time)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Ocean and Weather Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.eda_helper_functions import plot_coordinates\n",
        "\n",
        "plot_coordinates(df_ocean, df_weather, df_insitu, save_png=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distributions of the Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster df_ocean into K = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ocean = load_ocean_data(ocean_data_path, OCEAN_POINTS, verbose=False)\n",
        "df_ocean = process_df(df_ocean, drop_cols=[\"depth\"], verbose=False)\n",
        "\n",
        "df_weather = load_weather_data(weather_data_path, WEATHER_POINTS, verbose=False)\n",
        "df_weather = process_df(df_weather, verbose=False)\n",
        "\n",
        "df_insitu = load_insitu_data(verbose=False)\n",
        "df_insitu = process_flensburg_data(df_insitu, \n",
        "                                      start_time=df_ocean['time'].min(),\n",
        "                                      end_time=df_ocean['time'].max(),\n",
        "                                      verbose=False)\n",
        "\n",
        "df_insitu = group_data_hourly(df_insitu)\n",
        "df_insitu = process_df(df_insitu, drop_cols=[\"deph\"], verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "\n",
        "def cluster_df(df:pd.DataFrame, columns:list, n_clusters:int=3, display:bool=False, start_date=None, end_date=None):\n",
        "    \"\"\"\n",
        "    Clusters the DataFrame using KMeans clustering.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame to cluster.\n",
        "        columns (list): List of columns to use for clustering.\n",
        "        n_clusters (int): Number of clusters.\n",
        "        start_date (str): Optional start date for filtering data.\n",
        "        end_date (str): Optional end date for filtering data.\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with cluster labels.\n",
        "    \"\"\"\n",
        "    # === 1. Features auswählen (außer Zeit, da KMeans keine Zeit versteht)\n",
        "\n",
        "    df = df.copy() \n",
        "\n",
        "    columns = df.columns.tolist()\n",
        "    if start_date is not None:\n",
        "        df = df[df['time'] >= start_date]\n",
        "    if end_date is not None:\n",
        "        df = df[df['time'] <= end_date]\n",
        "    if \"time\" in columns:\n",
        "        columns.remove(\"time\")\n",
        "\n",
        "    # groupby latitude and longitude\n",
        "    df = df.groupby(['latitude', 'longitude']).mean().reset_index()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    X = df[columns]\n",
        "\n",
        "    # Standardisieren\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # KMeans-Clustering (k=n_clusters)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    # Cluster-Labels zurück ins DataFrame\n",
        "    df_clustered = df.loc[X.index].copy()\n",
        "    df_clustered['cluster'] = clusters\n",
        "\n",
        "    if display:\n",
        "        # Mittelpunkt berechnen für Basemap\n",
        "        mean_lat = df_clustered['latitude'].mean()\n",
        "        mean_lon = df_clustered['longitude'].mean()\n",
        "\n",
        "        # Karte mit Basemap zeichnen\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        m = Basemap(\n",
        "            projection='lcc',\n",
        "            resolution='i',\n",
        "            lat_0=mean_lat,\n",
        "            lon_0=mean_lon,\n",
        "            width=1.2e6,\n",
        "            height=1.2e6,\n",
        "        )\n",
        "\n",
        "        m.drawcoastlines()\n",
        "        m.drawcountries()\n",
        "        m.drawmapboundary(fill_color='lightblue')\n",
        "        m.fillcontinents(color='beige', lake_color='lightblue')\n",
        "\n",
        "        # Farben definieren\n",
        "        colors = ['red', 'green', 'orange', 'purple', 'pink']\n",
        "\n",
        "        print(df_clustered['cluster'].value_counts())\n",
        "        # Punkte plotten\n",
        "        for cluster_id in range(n_clusters):\n",
        "            print(f\"Cluster {cluster_id}: {len(df_clustered[df_clustered['cluster'] == cluster_id])} points\")\n",
        "            cluster_data = df_clustered[df_clustered['cluster'] == cluster_id]\n",
        "            x, y = m(cluster_data['longitude'].values, cluster_data['latitude'].values)\n",
        "            m.scatter(x, y, s=10, c=colors[cluster_id % len(colors)], label=f'Cluster {cluster_id}', alpha=0.6)\n",
        "\n",
        "        plt.legend(loc='upper left')\n",
        "        plt.title(f'KMeans Clustering (k={n_clusters}) der Ozeandaten')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    return df_clustered\n",
        "\n",
        "\n",
        "df_clustered = cluster_df(df_ocean, ['latitude', 'longitude', 'sla'], n_clusters=3, display=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of SLEV and SLA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ocean = load_ocean_data(ocean_data_path, OCEAN_POINTS, verbose=False)\n",
        "df_ocean = process_df(df_ocean, drop_cols=[\"depth\"], verbose=False)\n",
        "\n",
        "df_weather = load_weather_data(weather_data_path, WEATHER_POINTS, verbose=False)\n",
        "df_weather = process_df(df_weather, verbose=False)\n",
        "\n",
        "df_insitu = load_insitu_data(verbose=False)\n",
        "df_insitu = process_flensburg_data(df_insitu, \n",
        "                                      start_time=df_ocean['time'].min(),\n",
        "                                      end_time=df_ocean['time'].max(),\n",
        "                                      verbose=False)\n",
        "\n",
        "df_insitu = group_data_hourly(df_insitu)\n",
        "df_insitu = process_df(df_insitu, drop_cols=[\"deph\"], verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# find the closest location in df_ocean to the target location\n",
        "def find_closest_location(df: pd.DataFrame, target_lat: float, target_lon: float) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Find the closest location in the DataFrame to the target latitude and longitude.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame containing the data with 'latitude' and 'longitude'.\n",
        "        target_lat (float): Target latitude.\n",
        "        target_lon (float): Target longitude.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The row of the closest location.\n",
        "    \"\"\"\n",
        "    df[\"distance\"] = np.sqrt((df[\"latitude\"] - target_lat) ** 2 + (df[\"longitude\"] - target_lon) ** 2)\n",
        "    return df.loc[df[\"distance\"].idxmin()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def plot_closest_location(df_ocean: pd.DataFrame, sub_box:dict, target_lat: float, target_lon: float, save:bool=False) -> None:\n",
        "    \"\"\"\n",
        "    Plot the closest location in ocean data to the target latitude and longitude,\n",
        "    with a basemap background using Basemap.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    \n",
        "    # long: 9 - 12\n",
        "# lat: 54.5 - 55\n",
        "\n",
        "    # closest_location = find_closest_location(df_ocean, target_lat, target_lon)\n",
        "    df_subset = df_ocean[\n",
        "        (df_ocean[\"latitude\"] >= sub_box[\"lat_min\"]) &\n",
        "        (df_ocean[\"latitude\"] <= sub_box[\"lat_max\"]) &\n",
        "        (df_ocean[\"longitude\"] >= sub_box[\"lon_min\"]) &\n",
        "        (df_ocean[\"longitude\"] <= sub_box[\"lon_max\"])\n",
        "    ]\n",
        "    \n",
        "\n",
        "    # Mittelwerte für Kartenzentrum\n",
        "    mean_lat = df_ocean[\"latitude\"].mean()\n",
        "    mean_lon = df_ocean[\"longitude\"].mean()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Erstelle Basemap\n",
        "    m = Basemap(\n",
        "        projection='lcc',\n",
        "        resolution='i',\n",
        "        lat_0=mean_lat,\n",
        "        lon_0=mean_lon,\n",
        "        width=1.2e6,\n",
        "        height=1.2e6,\n",
        "    )\n",
        "\n",
        "    # Kartenelemente zeichnen\n",
        "    m.drawcoastlines()\n",
        "    m.drawcountries()\n",
        "    m.drawstates()\n",
        "    m.drawmapboundary(fill_color=\"lightblue\")\n",
        "    m.fillcontinents(color=\"lightgray\", lake_color=\"lightblue\")\n",
        "\n",
        "    # Koordinaten konvertieren (long, lat) → Karten-Koordinaten (x, y)\n",
        "    x_ocean, y_ocean = m(df_ocean[\"longitude\"].values, df_ocean[\"latitude\"].values)\n",
        "    x_target, y_target = m(target_lon, target_lat)\n",
        "    x_closest, y_closest = m(df_subset.longitude, df_subset.latitude)\n",
        "\n",
        "    # Punkte plotten\n",
        "    m.scatter(x_ocean, y_ocean, color=\"blue\", label=\"Ocean Data\", s=50)\n",
        "    m.scatter(x_target, y_target, color=\"green\", marker=\"*\", label=\"Target Point\", s=250)\n",
        "    m.scatter(x_closest, y_closest, color=\"orange\", marker=\"o\", label=\"Closest Points\", s=50)\n",
        "\n",
        "    # draw rectangle around the subset\n",
        "    # x_box = [sub_box[\"lon_min\"], sub_box[\"lon_max\"], sub_box[\"lon_max\"], sub_box[\"lon_min\"], sub_box[\"lon_min\"]]\n",
        "    # y_box = [sub_box[\"lat_min\"], sub_box[\"lat_min\"], sub_box[\"lat_max\"], sub_box[\"lat_max\"], sub_box[\"lat_min\"]]\n",
        "    # x_box, y_box = m(x_box, y_box)\n",
        "    # m.plot(x_box, y_box, color=\"green\", linestyle=\"--\", linewidth=2, label=\"Region of Interest\")\n",
        "    \n",
        "\n",
        "    #plt.title(\"Closest Location in Ocean Data\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    m.drawparallels(np.arange(-360, 360, 2), labels=[1, 0, 0, 0])\n",
        "    m.drawmeridians(np.arange(-360, 360, 2), labels=[0, 0, 0, 1])\n",
        "    if save:\n",
        "        plt.savefig(\"../thesis_plots/closest_location_ocean_data.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "sub_box = {\n",
        "        \"lat_min\": 54.5, # 54.4\n",
        "        \"lat_max\": 55.1, # 55.5\n",
        "        \"lon_min\": 9.9,\n",
        "        \"lon_max\": 10.1 # 10.5\n",
        "        }\n",
        "plot_closest_location(df_ocean=df_ocean, sub_box=sub_box, target_lat=LAT_FLENSBURG, target_lon=LON_FLENSBURG, save=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comparison_slev_sla(df_ocean: pd.DataFrame, df_insitu: pd.DataFrame, sub_box: dict, target_lat: float = 54.5, target_lon: float = 10.0, save: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Compare the SLEV and SLA data by plotting them on the same graph.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter the data for the closest location\n",
        "\n",
        "    df_ocean_target = df_ocean[\n",
        "                (df_ocean[\"latitude\"] >= sub_box[\"lat_min\"]) &\n",
        "                (df_ocean[\"latitude\"] <= sub_box[\"lat_max\"]) &\n",
        "                (df_ocean[\"longitude\"] >= sub_box[\"lon_min\"]) &\n",
        "                (df_ocean[\"longitude\"] <= sub_box[\"lon_max\"])\n",
        "            ].reset_index(drop=True)\n",
        "    \n",
        "    # Calculate the mean SLA \n",
        "    df_ocean_target = df_ocean_target.groupby(\"time\").mean().reset_index()\n",
        "\n",
        "    # Calculating Pearson correlation of SLEV and SLA\n",
        "    df_corr_ocean = df_ocean_target.copy()\n",
        "    df_corr_insitu = df_insitu.copy()\n",
        "\n",
        "    df_corr_ocean.index = pd.to_datetime(df_corr_ocean.index)\n",
        "    df_corr_insitu.index = pd.to_datetime(df_corr_insitu.index)\n",
        "    df_corr = pd.merge(df_corr_ocean[['sla']], df_corr_insitu[['slev']], left_index=True, right_index=True, how='inner')\n",
        "    corr = df_corr['slev'].corr(df_corr['sla'])\n",
        "    rmse = np.sqrt(np.mean((df_corr['slev'] - df_corr['sla']) ** 2))\n",
        "\n",
        "    # Plotting SLEV and SLA\n",
        "    alpha = 0.8\n",
        "    window_size = 24\n",
        "\n",
        "    rolling_mean_slev = df_insitu[\"slev\"].rolling(window=window_size).mean()\n",
        "    rolling_mean_sla = df_ocean_target[\"sla\"].rolling(window=window_size).mean()\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df_insitu[\"time\"], df_insitu[\"slev\"], label=\"water level Flensburg (SLEV)\", color=\"#0072B2\", alpha=0.1)\n",
        "    plt.plot(df_ocean[\"time\"], df_ocean[\"sla\"], label=\"water level - closests point (SLA)\", color=\"#E69F00\", alpha=0.1)\n",
        "    plt.plot(df_insitu[\"time\"], rolling_mean_slev, label=\"water level Flensburg (SLEV) Rolling Mean\", color=\"#0072B2\", linestyle=\"-\", alpha=alpha)\n",
        "    plt.plot(df_ocean_target[\"time\"], rolling_mean_sla, label=\"water level - closest points (SLA) Rolling Mean\", color=\"#E69F00\", linestyle=\"-\", alpha=alpha)\n",
        "    # plot text with the correlation value\n",
        "\n",
        "    plt.title(f\"Comparison of SLEV and SLA \\n Correlation: {corr:.2f}, RMSE: {rmse:.2f}\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Water Level [m]\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save:\n",
        "        plt.savefig(\"../data/plots/comparison_slev_sla.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # calcualte autocorrelation for SLEV and SLA for different lags\n",
        "    \n",
        "    # df_corr_insitu = df_corr_insitu.loc[(df_corr_insitu['time'] >= df_corr_ocean['time'].min()) & (df_corr_insitu['time'] <= df_corr_ocean['time'].max())]\n",
        "    # fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    # plot_acf(df_insitu[\"slev\"], lags=100, ax=ax[0])\n",
        "    # ax[0].set_title(\"Autocorrelation of SLEV\")\n",
        "    # plot_acf(df_ocean_target[\"sla\"], lags=100, ax=ax[1])\n",
        "    # ax[1].set_title(\"Autocorrelation of SLA\")\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "comparison_slev_sla(df_ocean=df_ocean, df_insitu=df_insitu, target_lat=LAT_FLENSBURG, target_lon=LON_FLENSBURG, sub_box=sub_box, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beide Zeitreihen zeigen eine starke Autokorrelation, was typisch für ozeanografische Zeitreihen ist. Die realen Daten (SLEV) wirken jedoch etwas unregelmäßiger und zeigen vermutlich tidenbedingte Oszillationen, die das Modell (SLA) nur gedämpft oder geglättet abbildet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wie verhält sich Wind, Windrichtung bei den unterschiedlichen Sturmfluten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ocean = load_ocean_data(ocean_data_path, OCEAN_POINTS, verbose=False)\n",
        "df_ocean = process_df(df_ocean, drop_cols=[\"depth\"], verbose=False)\n",
        "\n",
        "df_weather = load_weather_data(weather_data_path, WEATHER_POINTS, verbose=False)\n",
        "df_weather = process_df(df_weather, verbose=False)\n",
        "\n",
        "df_insitu = load_insitu_data(verbose=False)\n",
        "df_insitu = process_flensburg_data(df_insitu, \n",
        "                                      start_time=df_ocean['time'].min(),\n",
        "                                      end_time=df_ocean['time'].max(),\n",
        "                                      verbose=False)\n",
        "\n",
        "df_insitu = group_data_hourly(df_insitu)\n",
        "df_insitu = process_df(df_insitu, drop_cols=[\"deph\"], verbose=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cartopy.feature as cfeature\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shapely.geometry\n",
        "import xarray as xr\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.interpolate import griddata\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Funktion zur Landprüfung mit Cartopy\n",
        "def is_on_land(lon, lat):\n",
        "    land = cfeature.NaturalEarthFeature(\"physical\", \"land\", \"10m\")\n",
        "    for geom in land.geometries():\n",
        "        if geom.contains(shapely.geometry.Point(lon, lat)):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# Funktion zum Erstellen der Landmaske\n",
        "def create_land_mask(lon_grid, lat_grid):\n",
        "    coords_list = [(lon, lat) for lat in lat_grid for lon in lon_grid]\n",
        "    mask_flat = Parallel(n_jobs=-1)(\n",
        "        delayed(lambda p: not is_on_land(*p))(p) for p in tqdm(coords_list)\n",
        "    )\n",
        "    return np.array(mask_flat).reshape(len(lat_grid), len(lon_grid))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def lineplot_storm_surge(df, column_name, timepoints, \n",
        "                         ax:plt.Axes=None, display=True, \n",
        "                         calculation:str='mean', show_number_lines:bool=True, \n",
        "                         title:str=None, display_sturm_surge:bool=False,\n",
        "                         show_legend:bool=False,\n",
        "                         save:bool=False,\n",
        "                         legend_outside:bool=True\n",
        "                         ):\n",
        "\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(20, 5))\n",
        "\n",
        "    timepoints_array = np.array(timepoints)\n",
        "    timepoint_min = timepoints_array.min()\n",
        "    timepoint_max = timepoints_array.max()\n",
        "    # Eingrenzen des Datenbereichs\n",
        "    df_plot = df.loc[\n",
        "        (df[\"time\"] >= timepoint_min - pd.Timedelta(hours=62)) & \n",
        "        (df[\"time\"] <= timepoint_max + pd.Timedelta(hours=62))\n",
        "    ].reset_index(drop=True)\n",
        "\n",
        "    if column_name in WEATHER_DICT or column_name in OCEAN_DICT:\n",
        "        # filter data in SUB_BOX\n",
        "        df_plot = df_plot[\n",
        "            (df_plot[\"latitude\"] >= SUB_BOX[\"lat_min\"]) &\n",
        "            (df_plot[\"latitude\"] <= SUB_BOX[\"lat_max\"]) &\n",
        "            (df_plot[\"longitude\"] >= SUB_BOX[\"lon_min\"]) &\n",
        "            (df_plot[\"longitude\"] <= SUB_BOX[\"lon_max\"])\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "        if calculation == 'mean':\n",
        "            df_plot = df_plot.groupby(\"time\").mean().reset_index()\n",
        "        elif calculation == 'max':\n",
        "            df_plot = df_plot.groupby(\"time\").max().reset_index()\n",
        "\n",
        "    # Hightlight a red section where the values of df_insitu are greater than 1\n",
        "    if display_sturm_surge:\n",
        "        df_insitu_plot = df_insitu.loc[df_insitu[\"time\"].between(timepoint_min, timepoint_max)].reset_index(drop=True)\n",
        "        # max_slev = df_insitu_plot[\"slev\"].max()\n",
        "        # max_slev_time = df_insitu_plot.loc[df_insitu_plot[\"slev\"] == max_slev, \"time\"].values[0]\n",
        "        # ax.axvline(x=max_slev_time, color=\"red\", linestyle=\"--\", alpha=0.6, label=\"Sturm Surge\")\n",
        "        \n",
        "\n",
        "        ax.fill_between(df_insitu_plot[\"time\"], df_plot[column_name].min(), df_plot[column_name].max() , where=(df_insitu_plot[\"slev\"] > 1), color='lightgrey', alpha=0.5, label='storm surge event')\n",
        "        # limit the y axis to the max value of df_insitu_plot[\"slev\"] + 1\n",
        "        #ax.set_ylim(df_plot[column_name].min() - 1, df_plot[column_name].max() + 1)\n",
        "        #ax.set_ylim(df_plot[column_name].min() - 0.1, df_plot[column_name].max() + 0.1)\n",
        "    \n",
        "    if column_name in INSITU_DICT:\n",
        "        # Highlight storm surge classes with colored bands\n",
        "        flood_levels = [\n",
        "            (1.0, 1.25, 'yellow', 'storm surge'),\n",
        "            (1.25, 1.5, 'orange', 'medium storm surge'),\n",
        "            (1.5, 2.0, 'red', 'heavy storm surge'),\n",
        "            (2.0, 3.5, 'darkred', 'very heavy storm surge'),\n",
        "        ]\n",
        "\n",
        "        for y0, y1, color, label in flood_levels:\n",
        "            ax.axhspan(y0, y1, facecolor=color, alpha=0.3, label=label)\n",
        "        \n",
        "        ax.set_ylim(df_plot['slev'].min() - 0.1, df_plot['slev'].max() + 0.1)\n",
        "\n",
        "    if column_name == 'wind_direction_10m':\n",
        "        # Highlight wind direction with colored bands\n",
        "        wind_directions = [\n",
        "            (0, 45, '#0000FF', 'north'),\n",
        "            (45, 135, '#FFFF00', 'east'),\n",
        "            (135, 225, '#FF0000', 'south'),\n",
        "            (225, 315, '#FFA500', 'west'),\n",
        "            (315, 360, '#0000FF', 'north')\n",
        "        ]\n",
        "\n",
        "        for y0, y1, color, label in wind_directions:\n",
        "            ax.axhspan(y0, y1, facecolor=color, alpha=0.3, label=label)\n",
        "\n",
        "    if column_name == \"vo\":\n",
        "        # Highlight water directions with colored bands >0 then #0000FF <0 then #FF0000\n",
        "        ax.axhspan(0, df_plot[column_name].max(), facecolor='#0000FF', alpha=0.3, label='north direction')\n",
        "        ax.axhspan(df_plot[column_name].min(), 0, facecolor='#FF0000', alpha=0.3, label='south direction')\n",
        "        \n",
        "    if column_name == \"uo\":\n",
        "        # Highlight water directions with colored bands >0 then #FFFF00 <0 then #FFA500\n",
        "        ax.axhspan(0, df_plot[column_name].max(), facecolor='#FFFF00', alpha=0.3, label='east direction')\n",
        "        ax.axhspan(df_plot[column_name].min(), 0, facecolor='#FFA500', alpha=0.3, label='west direction')\n",
        "\n",
        "    if column_name == \"wo\":\n",
        "        # Highlight water directions with colored bands >0 then #00CED1 <0 then #2F4F4F\n",
        "        ax.axhspan(0, df_plot[column_name].max(), facecolor='#00CED1', alpha=0.3, label='upward direction')\n",
        "        ax.axhspan(df_plot[column_name].min(), 0, facecolor='#2F4F4F', alpha=0.3, label='downward direction')\n",
        "\n",
        "    # highlight low and high pressure\n",
        "    # if column_name == \"pressure_msl\":\n",
        "    #     # Highlight low and high pressure with colored bands\n",
        "    #     low_pressure = (980, 990, '#00CED1', 'Low Pressure')\n",
        "    #     high_pressure = (1040, 1050, '#2F4F4F', 'High Pressure')\n",
        "\n",
        "    #     ax.axhspan(low_pressure[0], low_pressure[1], facecolor=low_pressure[2], alpha=0.3, label=low_pressure[3])\n",
        "    #     ax.axhspan(high_pressure[0], high_pressure[1], facecolor=high_pressure[2], alpha=0.3, label=high_pressure[3])\n",
        "\n",
        "    if column_name == \"wind_speed_10m\" or column_name == \"wind_gusts_10m\":\n",
        "    # Classification based on Beaufort scale (km/h) with color bands\n",
        "        wind_bands = [\n",
        "            (0, 5, '#00FF00', 'calm (0)'),                   # bright green\n",
        "            (5, 11, '#32CD32', 'light air (1)'),             # lime green\n",
        "            (11, 19, '#7FFF00', 'light breeze (2)'),         # chartreuse\n",
        "            (19, 28, '#ADFF2F', 'gentle breeze (3)'),        # green-yellow\n",
        "            (28, 38, '#FFFF00', 'moderate breeze (4)'),      # yellow\n",
        "            (38, 49, '#FFD700', 'fresh breeze (5)'),         # gold\n",
        "            (49, 61, '#FFA500', 'strong breeze (6)'),        # orange\n",
        "            (61, 74, '#FF8C00', 'near gale (7)'),            # dark orange\n",
        "            (74, 88, '#FF4500', 'gale (8)'),                 # orange red\n",
        "            (88, 102, '#FF0000', 'strong gale (9)'),         # red\n",
        "            (102, 117, '#B22222', 'storm (10)'),             # firebrick\n",
        "            (117, 150, '#8B0000', 'violent storm (11–12+)')  # dark red\n",
        "        ]\n",
        "\n",
        "\n",
        "        for band in wind_bands:\n",
        "            ax.axhspan(band[0], band[1], facecolor=band[2], alpha=0.3, label=band[3])\n",
        "        \n",
        "        ax.set_ylim(df_plot[column_name].min() - 0.5, df_plot[column_name].max() + 0.5)\n",
        "        # set legend title\n",
        "    \n",
        "\n",
        "    # Plot SLEV\n",
        "    x = df_plot[\"time\"]\n",
        "    y = df_plot[column_name]\n",
        "    ax.plot(x, y, label=column_name, color=\"royalblue\", linewidth=2)\n",
        "\n",
        "\n",
        "    # VLines + Text\n",
        "    if show_number_lines:\n",
        "        for i, t in enumerate(timepoints):\n",
        "            ax.axvline(x=t, ymin=0, ymax=1, color=\"crimson\", linestyle=\"--\", alpha=0.6)\n",
        "            # ax.text(\n",
        "            #     t,\n",
        "            #     y.mean(),\n",
        "            #     t.strftime(\"%Y-%m-%d %H:%M\"),\n",
        "            #     color=\"crimson\",\n",
        "            #     fontsize=9,\n",
        "            #     ha=\"center\",\n",
        "            #     rotation=90,\n",
        "            #     rotation_mode=\"anchor\"\n",
        "            # )\n",
        "            \n",
        "            ax.annotate(f\"{i+1}\",  # Changed from {number} to {i+1} to display the index\n",
        "                    xy=(t, y.mean()),\n",
        "                    xytext=(t, y.mean()),\n",
        "                    xycoords=\"data\", \n",
        "                    #fontsize=12, \n",
        "                    ha=\"left\", \n",
        "                    va=\"top\", \n",
        "                    color=\"black\", \n",
        "                    bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"),\n",
        "                    zorder=10\n",
        "                    )\n",
        "    # Format X-Achse\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
        "    ax.xaxis.set_major_locator(mdates.HourLocator(interval=24)) \n",
        "    plt.xticks(rotation=30, ha=\"right\")\n",
        "\n",
        "    # Labels & Title\n",
        "    #ax.set_xlabel(\"Time\", fontsize=12)\n",
        "    if title is None:\n",
        "\n",
        "        if column_name in INSITU_DICT:\n",
        "            y_label = f\"{column_name} [{INSITU_DICT[column_name]['unit']}]\"\n",
        "            title = f\"{INSITU_DICT[column_name]['description']} ({column_name}) of Flensburg \\n{INSITU_DICT[column_name]['explanation']}\"\n",
        "            # subt\n",
        "        elif column_name in OCEAN_DICT:\n",
        "            y_label = f\"{column_name} [{OCEAN_DICT[column_name]['unit']}]\"\n",
        "            title = f\"{calculation.capitalize()} of {OCEAN_DICT[column_name]['description']} ({column_name}) Timegraph \\n{OCEAN_DICT[column_name]['explanation']}\"\n",
        "        elif column_name in WEATHER_DICT:\n",
        "            y_label = f\"{column_name} [{WEATHER_DICT[column_name]['unit']}]\"\n",
        "            title = f\"{calculation.capitalize()} of {WEATHER_DICT[column_name]['description']} ({column_name}) Timegraph \\n{WEATHER_DICT[column_name]['explanation']}\"\n",
        "        else:\n",
        "            y_label = f\"{column_name}\"\n",
        "            title = f\"{column_name}\"\n",
        "    else:\n",
        "        if column_name in INSITU_DICT:\n",
        "            y_label = f\"{column_name} [{INSITU_DICT[column_name]['unit']}]\"\n",
        "        elif column_name in OCEAN_DICT:\n",
        "            y_label = f\"{column_name} [{OCEAN_DICT[column_name]['unit']}]\"\n",
        "        elif column_name in WEATHER_DICT:\n",
        "            y_label = f\"{column_name} [{WEATHER_DICT[column_name]['unit']}]\"\n",
        "        else:\n",
        "            y_label = f\"{column_name}\"\n",
        "\n",
        "\n",
        "    # ax.set_title(f\"{title}\", \n",
        "    #              #fontsize=14, \n",
        "    #              pad=15)\n",
        "    ax.set_ylabel(f\"{y_label}\", \n",
        "                  #fontsize=12\n",
        "                  )\n",
        "        \n",
        "\n",
        "    # for t in timepoints:\n",
        "    #     ax.axvline(x=t, ymin=0, ymax=1, color=\"crimson\", linestyle=\"--\", alpha=0.6, label=\"Storm Surge\")\n",
        "\n",
        "    # # Nur einmaliger Eintrag\n",
        "    # handles, labels = ax.get_legend_handles_labels()\n",
        "    # by_label = dict(zip(labels, handles))\n",
        "    # ax.legend(by_label.values(), by_label.keys(), loc=\"upper left\")\n",
        "\n",
        "\n",
        "    # Grid, Legend, Layout\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    #ax.legend(loc=\"upper left\")\n",
        "    if show_legend and legend_outside:\n",
        "        ax.legend(\n",
        "                #title=\"Beaufort Scale\",\n",
        "                #loc=\"upper left\",\n",
        "                #fontsize=12,\n",
        "                #title_fontsize=10,\n",
        "                framealpha=0.0,\n",
        "                facecolor='white',\n",
        "                #edgecolor='gray',\n",
        "                ncol=1,\n",
        "                fancybox=True,\n",
        "                shadow=False,\n",
        "                borderaxespad=0.3,\n",
        "                bbox_to_anchor=(1.01, 1),\n",
        "            )\n",
        "    if show_legend and not legend_outside:\n",
        "        ax.legend()\n",
        "    \n",
        "    #plt.tight_layout()\n",
        "    if save:\n",
        "        plt.savefig(f\"../data/plots/storm_surge_line_plot_{column_name}_{timepoints[0].strftime('%Y%m%d')}-{timepoints[-1].strftime('%Y%m%d')}.png\", dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved plot as ../data/plots/storm_surge_line_plot_{column_name}_{timepoints[0].strftime('%Y%m%d')}_{timepoints[-1].strftime('%Y%m%d')}.png\")\n",
        "\n",
        "    if display:\n",
        "        plt.show()\n",
        "    \n",
        "\n",
        "    return ax\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_ocean_target = df_ocean[\n",
        "#             (df_ocean[\"latitude\"] >= sub_box[\"lat_min\"]) &\n",
        "#             (df_ocean[\"latitude\"] <= sub_box[\"lat_max\"]) &\n",
        "#             (df_ocean[\"longitude\"] >= sub_box[\"lon_min\"]) &\n",
        "#             (df_ocean[\"longitude\"] <= sub_box[\"lon_max\"])\n",
        "#         ].reset_index(drop=True)\n",
        "\n",
        "# # Calculate the mean SLA \n",
        "# df_ocean_target = df_ocean_target.groupby(\"time\").mean().reset_index()\n",
        "\n",
        "# # Calculating Pearson correlation of SLEV and SLA\n",
        "# df_corr_ocean = df_ocean_target.copy()\n",
        "# df_corr_insitu = df_insitu.copy()\n",
        "\n",
        "# df_corr_ocean.index = pd.to_datetime(df_corr_ocean.index)\n",
        "# df_corr_insitu.index = pd.to_datetime(df_corr_insitu.index)\n",
        "# df_corr = pd.merge(df_corr_ocean[['sla']], df_corr_insitu[['slev']], left_index=True, right_index=True, how='inner')\n",
        "# corr = df_corr['slev'].corr(df_corr['sla'])\n",
        "# rmse = np.sqrt(np.mean((df_corr['slev'] - df_corr['sla']) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import scipy\n",
        "\n",
        "def grid_to_xarray(key, grid, lon_grid, lat_grid):\n",
        "    ds = xr.Dataset(\n",
        "            {\n",
        "                f\"{key}\": ((\"latitude\", \"longitude\"), grid),\n",
        "\n",
        "            },\n",
        "            coords={\n",
        "                \"latitude\": lat_grid,\n",
        "                \"longitude\": lon_grid,\n",
        "            },\n",
        "        )\n",
        "    return ds \n",
        "\n",
        "\n",
        "def plot_for_timepoint(timepoint, ax=None, grid_size_ocean=50, \n",
        "                       wind_grid_size=20, vmin=-1.0, vmax=1.5, \n",
        "                       plot_water_velocity_data=True, plot_wind_data=True, \n",
        "                       number=None, cluster:bool=False, timepoints:list=None,\n",
        "                       show_cluster:bool=False,\n",
        "                       ):\n",
        "    \n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    df_weather_time = df_weather[df_weather[\"time\"] == timepoint]\n",
        "    df_ocean_time = df_ocean[df_ocean[\"time\"] == timepoint]\n",
        "\n",
        "    if cluster and timepoints is not None:\n",
        "        # cluster the ocean data #['latitude', 'longitude', 'sla']\n",
        "        timepoints = sorted(timepoints)\n",
        "        print(f\"Clustering for timepoints: {timepoints}\")\n",
        "        df_cluster = cluster_df(df_ocean, ['latitude', 'longitude', 'sla'], n_clusters=3, display=show_cluster, start_date=timepoints[0], end_date=timepoints[-1])\n",
        "\n",
        "\n",
        "        # find the cluster number to closest point of Flensburg\n",
        "        closest_point = find_closest_location(df_cluster, target_lat=54.8, target_lon=10)\n",
        "\n",
        "\n",
        "        df_cluster = df_cluster[df_cluster[\"cluster\"] == closest_point[\"cluster\"]].reset_index(drop=True)\n",
        "        # display(df_cluster)\n",
        "        # print(df_cluster.cluster.unique())\n",
        "\n",
        "        # filter df_weather_time to the latitude and longitude which are close to latitude and longitude of df_cluster\n",
        "        df_weather_time_cluster = df_weather_time[\n",
        "            (df_weather_time[\"latitude\"] >= df_cluster[\"latitude\"].min()) &\n",
        "            (df_weather_time[\"latitude\"] <= df_cluster[\"latitude\"].max()) &\n",
        "            (df_weather_time[\"longitude\"] >= df_cluster[\"longitude\"].min()) &\n",
        "            (df_weather_time[\"longitude\"] <= df_cluster[\"longitude\"].max())\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "        mean_wind_speed = df_weather_time_cluster[\"wind_speed_10m\"].mean()\n",
        "        print(f\"Mean wind speed in cluster: {mean_wind_speed}\")\n",
        "\n",
        "        df_ocean_time_cluster = df_ocean_time[\n",
        "            (df_ocean_time[\"latitude\"] >= df_cluster[\"latitude\"].min()) &\n",
        "            (df_ocean_time[\"latitude\"] <= df_cluster[\"latitude\"].max()) &\n",
        "            (df_ocean_time[\"longitude\"] >= df_cluster[\"longitude\"].min()) &\n",
        "            (df_ocean_time[\"longitude\"] <= df_cluster[\"longitude\"].max())\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "        # calculate the actual velocity with time included\n",
        "        df_ocean_time_cluster['velocity'] = np.sqrt(df_ocean_time_cluster[\"uo\"]**2 + df_ocean_time_cluster[\"vo\"]**2)\n",
        "\n",
        "        mean_ocean_velocity = df_ocean_time_cluster[\"velocity\"].mean()\n",
        "        print(f\"Mean ocean velocity in cluster: {mean_ocean_velocity}\")\n",
        "\n",
        "\n",
        "\n",
        "    # add rmse to df_ocean_time[sla]\n",
        "    #df_ocean_time[\"sla\"] += rmse\n",
        "\n",
        "    lon_grid = np.linspace(df_ocean_time[\"longitude\"].min(), df_ocean_time[\"longitude\"].max(), grid_size_ocean)\n",
        "    lat_grid = np.linspace(df_ocean_time[\"latitude\"].min(), df_ocean_time[\"latitude\"].max(), grid_size_ocean)\n",
        "    lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)\n",
        "\n",
        "    sla_grid = griddata(\n",
        "        (df_ocean_time[\"longitude\"], df_ocean_time[\"latitude\"]),\n",
        "        df_ocean_time[\"sla\"],\n",
        "        (lon_mesh, lat_mesh),\n",
        "        method=\"linear\",\n",
        "    )\n",
        "\n",
        "    m = Basemap(\n",
        "        projection=\"cyl\",\n",
        "        resolution=\"i\",\n",
        "        llcrnrlon=lon_grid.min(),\n",
        "        urcrnrlon=lon_grid.max(),\n",
        "        llcrnrlat=lat_grid.min(),\n",
        "        urcrnrlat=lat_grid.max(),\n",
        "        ax=ax,\n",
        "    )\n",
        "    m.fillcontinents(color=\"grey\", lake_color=\"white\", alpha=0.5)\n",
        "    m.drawcoastlines()\n",
        "    m.drawcountries()\n",
        "\n",
        "    # plot target point\n",
        "    x_target, y_target = m(LON_FLENSBURG, LAT_FLENSBURG)  # Reihenfolge: (longitude, latitude)\n",
        "    m.scatter(x_target, y_target, color=\"green\", marker=\"*\", label=\"Flensburg\", s=200) \n",
        "\n",
        "    if cluster:\n",
        "        x_box = [df_cluster[\"longitude\"].min(), df_cluster[\"longitude\"].max(), df_cluster[\"longitude\"].max(), df_cluster[\"longitude\"].min(), df_cluster[\"longitude\"].min()]\n",
        "        y_box = [df_cluster[\"latitude\"].min()+0.1, df_cluster[\"latitude\"].min()+0.1, df_cluster[\"latitude\"].max(), df_cluster[\"latitude\"].max(), df_cluster[\"latitude\"].min()+0.1]\n",
        "        x_box, y_box = m(x_box, y_box)\n",
        "        ax.plot(x_box, y_box, color=\"green\", linestyle=\"--\", linewidth=2, label=\"box of interest \")\n",
        "    else:\n",
        "        # plot rectangle around target point\n",
        "        x_box = [SUB_BOX[\"lon_min\"], SUB_BOX[\"lon_max\"], SUB_BOX[\"lon_max\"], SUB_BOX[\"lon_min\"], SUB_BOX[\"lon_min\"]]\n",
        "        y_box = [SUB_BOX[\"lat_min\"], SUB_BOX[\"lat_min\"], SUB_BOX[\"lat_max\"], SUB_BOX[\"lat_max\"], SUB_BOX[\"lat_min\"]]\n",
        "        x_box, y_box = m(x_box, y_box)\n",
        "        ax.plot(x_box, y_box, color=\"green\", linestyle=\"--\", linewidth=2, label=\"box of interest \")\n",
        "        #ax.fill(x_box, y_box, color=\"white\", alpha=0.2)\n",
        "\n",
        "\n",
        "    # Create ocean grid\n",
        "    mask = create_land_mask(lon_grid, lat_grid)\n",
        "    sla_grid[~mask] = np.nan\n",
        "\n",
        "    x_mesh, y_mesh = m(lon_mesh, lat_mesh)\n",
        "    heatmap = m.pcolormesh(x_mesh, y_mesh, sla_grid, cmap=\"magma\", shading=\"auto\", vmin=vmin, vmax=vmax)\n",
        "\n",
        "\n",
        "    # Wasser Geschwindigkeitsdaten\n",
        "    # eastward and northward velocity\n",
        "    if plot_water_velocity_data:\n",
        "        water_uo = griddata(\n",
        "            (df_ocean_time[\"longitude\"], df_ocean_time[\"latitude\"]),\n",
        "            df_ocean_time[\"uo\"],\n",
        "            (lon_mesh, lat_mesh),\n",
        "            method=\"linear\",\n",
        "        )\n",
        "        water_vo = griddata(\n",
        "            (df_ocean_time[\"longitude\"], df_ocean_time[\"latitude\"]),\n",
        "            df_ocean_time[\"vo\"],\n",
        "            (lon_mesh, lat_mesh),\n",
        "            method=\"linear\",\n",
        "        )\n",
        "\n",
        "        # stride depends on the grid size\n",
        "        if grid_size_ocean <= 100:\n",
        "            stride = 1\n",
        "        elif grid_size_ocean <= 200:\n",
        "            stride = 4\n",
        "        elif grid_size_ocean <= 300:\n",
        "            stride = 6\n",
        "        elif grid_size_ocean <= 400:\n",
        "            stride = 8\n",
        "        elif grid_size_ocean <= 500:\n",
        "            stride = 12\n",
        "        elif grid_size_ocean <= 600:\n",
        "            stride = 16\n",
        "        elif grid_size_ocean <= 700:\n",
        "            stride = 20\n",
        "        elif grid_size_ocean <= 800:\n",
        "            stride = 24\n",
        "\n",
        "        water_uo[~mask] = np.nan\n",
        "        water_vo[~mask] = np.nan\n",
        "        #stride = 2  \n",
        "        x_current = x_mesh[::stride, ::stride]\n",
        "        y_current = y_mesh[::stride, ::stride]\n",
        "        u_current = water_uo[::stride, ::stride]\n",
        "        v_current = water_vo[::stride, ::stride]\n",
        "\n",
        "        quiv_current = m.quiver(\n",
        "            x_current,\n",
        "            y_current,\n",
        "            u_current,\n",
        "            v_current,\n",
        "            scale=20,        # je nach Einheiten der uo/vo anpassen\n",
        "            color='grey',   # z.B. andere Farbe als Wind\n",
        "            width=0.002,    # dünner Pfeil\n",
        "            alpha=0.99,\n",
        "            label=\"water direction and velocity\",\n",
        "        )\n",
        "        # ax.quiverkey(quiv_current, 0.92, 0.04, 20, '1 m/s Current', labelpos='E', \n",
        "        #             coordinates='axes', \n",
        "        #             color='grey')\n",
        "\n",
        "        ds_ocean_uo = grid_to_xarray(\"vo\", water_uo, lon_grid, lat_grid)\n",
        "        ds_ocean_vo = grid_to_xarray(\"uo\", water_vo, lon_grid, lat_grid)\n",
        "\n",
        "        # merge the datasets\n",
        "        ds_ocean_uo_vo= xr.merge([ds_ocean_uo, ds_ocean_vo])\n",
        "\n",
        "        # calculate the actual velocity with time included\n",
        "        ds_ocean_uo_vo['velocity'] = np.sqrt(ds_ocean_uo_vo[\"uo\"]**2 + ds_ocean_uo_vo[\"vo\"]**2)\n",
        "\n",
        "        # select the ocean current at the bbox\n",
        "        ds_ocean_uo_vo_bbox = ds_ocean_uo_vo.sel(\n",
        "            latitude=slice(SUB_BOX[\"lat_min\"], SUB_BOX[\"lat_max\"]),\n",
        "            longitude=slice(SUB_BOX[\"lon_min\"], SUB_BOX[\"lon_max\"]),\n",
        "        )\n",
        "\n",
        "        if not cluster:\n",
        "            # get the mean ocean current in the ds_ocean_uo_vo_bbox\n",
        "            mean_ocean_velocity = ds_ocean_uo_vo_bbox[\"velocity\"].mean().values\n",
        "\n",
        "     # Winddaten   \n",
        "    if plot_wind_data:\n",
        "        \n",
        "        lon_grid_wind = np.linspace(df_weather_time[\"longitude\"].min(), df_weather_time[\"longitude\"].max(), wind_grid_size)\n",
        "        lat_grid_wind = np.linspace(df_weather_time[\"latitude\"].min(), df_weather_time[\"latitude\"].max(), wind_grid_size)\n",
        "        lon_mesh_wind, lat_mesh_wind = np.meshgrid(lon_grid_wind, lat_grid_wind)\n",
        "\n",
        "        wind_speed_grid = griddata(\n",
        "            (df_weather_time[\"longitude\"], df_weather_time[\"latitude\"]),\n",
        "            df_weather_time[\"wind_speed_10m\"],\n",
        "            (lon_mesh_wind, lat_mesh_wind),\n",
        "            method=\"linear\",\n",
        "        )\n",
        "        wind_dir_grid = griddata(\n",
        "            (df_weather_time[\"longitude\"], df_weather_time[\"latitude\"]),\n",
        "            df_weather_time[\"wind_direction_10m\"],\n",
        "            (lon_mesh_wind, lat_mesh_wind),\n",
        "            method=\"linear\",\n",
        "        )\n",
        "\n",
        "        u = wind_speed_grid * -np.cos(np.deg2rad(wind_dir_grid))\n",
        "        v = wind_speed_grid * -np.sin(np.deg2rad(wind_dir_grid))\n",
        "        x_wind, y_wind = m(lon_mesh_wind, lat_mesh_wind)\n",
        "        quiv_wind = m.quiver(x_wind, y_wind, u, v, scale=1500, color=\"black\")\n",
        "        #ax.quiverkey(quiv_wind, 0.92, 0.08, 10, '10 m/s Wind', labelpos='E', coordinates='axes', color='black')\n",
        "\n",
        "\n",
        "        # make xarray from wind_speed_grid and wind_dir_grid\n",
        "        ds_wind_speed = grid_to_xarray(\"wind_speed\", wind_speed_grid, lon_grid_wind, lat_grid_wind)\n",
        "        \n",
        "        # select the wind speed and direction at the bbox\n",
        "        ds_wind_speed_bbox = ds_wind_speed.sel(\n",
        "            latitude=slice(SUB_BOX[\"lat_min\"], SUB_BOX[\"lat_max\"]),\n",
        "            longitude=slice(SUB_BOX[\"lon_min\"], SUB_BOX[\"lon_max\"]),\n",
        "        )\n",
        "\n",
        "        if not cluster:\n",
        "            # get the mean wind speed in the ds_wind_bbox\n",
        "            mean_wind_speed = ds_wind_speed_bbox[\"wind_speed\"].mean().values\n",
        "        \n",
        "        # Add contour lines for pressure_msl\n",
        "    if \"pressure_msl\" in df_weather_time.columns:\n",
        "        pressure_msl_grid = griddata(\n",
        "            (df_weather_time[\"longitude\"], df_weather_time[\"latitude\"]),\n",
        "            df_weather_time[\"pressure_msl\"],\n",
        "            (lon_mesh_wind, lat_mesh_wind),\n",
        "            method=\"linear\",\n",
        "        )\n",
        "        if pressure_msl_grid is not None:\n",
        "            x_pressure, y_pressure = m(lon_mesh_wind, lat_mesh_wind)\n",
        "            # Konturstufen definieren (alle 2 hPa z.B.)\n",
        "            \n",
        "\n",
        "            # Konturstufen und Konturen zeichnen\n",
        "            levels = np.arange(np.nanmin(pressure_msl_grid), np.nanmax(pressure_msl_grid), 2)\n",
        "            cmap = plt.get_cmap('coolwarm')\n",
        "\n",
        "            cs = m.contour(x_pressure, y_pressure, pressure_msl_grid, \n",
        "                        levels=levels, \n",
        "                        cmap=cmap, \n",
        "                        linewidths=1.0)\n",
        "\n",
        "            plt.clabel(cs, inline=True, \n",
        "                       #fontsize=12, \n",
        "                       fmt=\"%.0f hPa\")\n",
        "\n",
        "\n",
        "            #cbar = plt.colorbar(cs, orientation='horizontal', pad=0.05)\n",
        "            #cbar.set_label('Luftdruck (hPa)')\n",
        "\n",
        "\n",
        "            # Tiefdruck: Lokale Minima, Hochdruck: Lokale Maxima\n",
        "            pressure_msl_masked = np.ma.masked_invalid(pressure_msl_grid)\n",
        "\n",
        "            # Lokale Minima und Maxima suchen\n",
        "            minima = (scipy.ndimage.minimum_filter(pressure_msl_masked, size=20, mode='nearest') == pressure_msl_masked)\n",
        "            maxima = (scipy.ndimage.maximum_filter(pressure_msl_masked, size=20, mode='nearest') == pressure_msl_masked)\n",
        "\n",
        "            # 4. Positionen der Minima und Maxima bestimmen\n",
        "            min_locs = np.where(minima)\n",
        "            max_locs = np.where(maxima)\n",
        "\n",
        "            # # \"L\" für Tiefdruck, \"H\" für Hochdruck auf Karte plotten\n",
        "            # for y_idx, x_idx in zip(*min_locs):\n",
        "            #     x, y = m(x_pressure[y_idx, x_idx], y_pressure[y_idx, x_idx])\n",
        "            #     plt.text(x, y, 'L', fontsize=15, fontweight='bold', ha='center', va='center', color='blue')\n",
        "\n",
        "            # for y_idx, x_idx in zip(*max_locs):\n",
        "            #     x, y = m(x_pressure[y_idx, x_idx], y_pressure[y_idx, x_idx])\n",
        "            #     plt.text(x, y, 'H', fontsize=15, fontweight='bold', ha='center', va='center', color='red')\n",
        "\n",
        "            # Nur starke Hochs und Tiefs markieren   \n",
        "            # for y_idx, x_idx in zip(*min_locs):\n",
        "            #     if pressure_msl_grid[y_idx, x_idx] < 1005:\n",
        "            #         x, y = m(x_pressure[y_idx, x_idx], y_pressure[y_idx, x_idx])\n",
        "            #         plt.text(x, y, 'L', fontsize=15, fontweight='bold', ha='center', va='center', color='blue')\n",
        "\n",
        "            # for y_idx, x_idx in zip(*max_locs):\n",
        "            #     if pressure_msl_grid[y_idx, x_idx] > 1015:\n",
        "            #         x, y = m(x_pressure[y_idx, x_idx], y_pressure[y_idx, x_idx])\n",
        "            #         plt.text(x, y, 'H', fontsize=15, fontweight='bold', ha='center', va='center', color='red')\n",
        "\n",
        "    ax.set_title(f\"Time: {pd.to_datetime(timepoint).strftime('%Y-%m-%d %H:%M')}\\n Mean wind speed in box of interest  {mean_wind_speed:.3f} km/h \\n Mean water velocity in box of interest {mean_ocean_velocity:.3f} m/s\", pad=15)\n",
        "    #ax.set_title(f\"Time: {pd.to_datetime(timepoint).strftime('%Y-%m-%d %H:%M')}\", fontsize=14, pad=15)\n",
        "\n",
        "    m.drawparallels(np.arange(0, 360, 2), labels=[1, 0, 0, 0])\n",
        "    m.drawmeridians(np.arange(0, 350, 2), labels=[0, 0, 0, 1])\n",
        "    \n",
        "    # anotate the number of the plot on the top left corner\n",
        "    if number is not None:\n",
        "        \n",
        "        number = str(int(number) + 1)\n",
        "\n",
        "        #x_anotate, y_annotate = m(10, 59)\n",
        "        plt.annotate(f\"{number}\", \n",
        "                     xy=(0.01, 0.98),\n",
        "                     xytext=(0.01, 0.98),\n",
        "                     xycoords=\"axes fraction\", \n",
        "                     #fontsize=12, \n",
        "                     ha=\"left\", \n",
        "                     va=\"top\", \n",
        "                     color=\"black\", \n",
        "                     bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"),\n",
        "                     zorder=10\n",
        "                     )\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "def analyse_storm_surges(title: str = \"Storm Surges Analysis\", timepoints: list = None, grid_size_ocean=50, wind_grid_size=20, plot_water_velocity_data=True, plot_wind_data=True, save=False):\n",
        "    \n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # Neue Anordnung: 2 Zeilen, 2 Spalten → Zeitreihe oben, darunter 2x2 Heatmaps\n",
        "    n_cols = 2  # Anzahl der Spalten\n",
        "    n_rows = (len(timepoints) + n_cols - 1) // n_cols  # Berechne die benötigten Zeilen\n",
        "    gs = gridspec.GridSpec(n_rows, n_cols, \n",
        "                           #height_ratios=[0.4] + [0.8] * (n_rows - 1), \n",
        "                           hspace=0.1, wspace=0.1)\n",
        "\n",
        "    heatmaps = []\n",
        "    axes_heatmaps = []\n",
        "\n",
        "    timepoints_array = np.array(timepoints)\n",
        "    timepoint_min = timepoints_array.min()\n",
        "    timepoint_max = timepoints_array.max()\n",
        "\n",
        "    # Eingrenzen des Datenbereichs\n",
        "    df_plot = df_ocean.loc[\n",
        "        (df_ocean[\"time\"] >= timepoint_min - pd.Timedelta(hours=62)) & \n",
        "        (df_ocean[\"time\"] <= timepoint_max + pd.Timedelta(hours=62))\n",
        "    ].reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "    # select the smallest sla and maximum sla\n",
        "    min_sla = df_plot[\"sla\"].min()\n",
        "    max_sla = df_plot[\"sla\"].max()\n",
        "    print(f\"Min SLA: {min_sla}, Max SLA: {max_sla}\")\n",
        "\n",
        "    # # Zeitverlauf ganz oben über beide Spalten\n",
        "    # ax_line = fig.add_subplot(gs[0, :])  # oberste Zeile, beide Spalten\n",
        "    # lineplot_storm_surge(df_insitu, timepoints, ax=ax_line)\n",
        "    # ax_line.set_title(\"Time Graph of Water Level Elevation in Flensburg\")\n",
        "    # ax_line.set_position([0.1, 0.75, 0.8, 0.2])  # Adjust size and position\n",
        "\n",
        "    \n",
        "    # 2x2 Heatmaps darunter\n",
        "    min_sla_round = np.round(min_sla, 1)\n",
        "    max_sla_round = np.round(max_sla, 1)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    for i, timepoint in enumerate(timepoints):\n",
        "        ax = fig.add_subplot(gs[i // n_cols, i % n_cols])  # Zeile und Spalte dynamisch berechnen\n",
        "        \n",
        "        #ax.set_position([0.1 + (i % n_cols) * 0.4, 0.1 + (i // n_cols) * 0.1, 0.35, 0.35])  # Adjust size and position [left, bottom, width, height]\n",
        "        axes_heatmaps.append(ax)\n",
        "        heatmap = plot_for_timepoint(timepoint=timepoint, ax=ax, \n",
        "                                     grid_size_ocean=grid_size_ocean, wind_grid_size=wind_grid_size, \n",
        "                                     vmin=min_sla_round, vmax=max_sla_round, \n",
        "                                     plot_water_velocity_data=plot_water_velocity_data, plot_wind_data=plot_wind_data, \n",
        "                                     number=i, \n",
        "                                     cluster=False, timepoints=timepoints,\n",
        "                                     )\n",
        "\n",
        "        #heatmap = plot_for_timepoint(timepoint, ax, grid_size_ocean, wind_grid_size, min_sla_round, max_sla_round, plot_water_velocity_data=plot_water_velocity_data, plot_wind_data=plot_wind_data, number=i, cluster=True, timepoints=timepoints)\n",
        "        heatmaps.append(heatmap)\n",
        "\n",
        "    # Gemeinsame Farbleiste rechts neben den Heatmaps\n",
        "    cbar_ax = fig.add_axes([0.32, 0.05, 0.55, 0.03])  # [left, bottom, width, height]\n",
        "    cbar = fig.colorbar(heatmaps[0], cax=cbar_ax, orientation=\"horizontal\", use_gridspec=True)\n",
        "    \n",
        "    ticks = np.linspace(min_sla_round, max_sla_round, num=5)  # Anzahl der Ticks anpassen\n",
        "    cbar.set_ticks(ticks)\n",
        "    cbar.set_label(\"Water Level (m)\")\n",
        "\n",
        "    legend_elements = []\n",
        "    legend_elements.append(Line2D([0], [0], color='green', lw=2, label='box of interest ', linestyle='--'))\n",
        "    \n",
        "    legend_elements.append(Line2D([0], [0], color='green', marker='*', markersize=15, linestyle='', label='Flensburg'))\n",
        "\n",
        "    \n",
        "    if plot_water_velocity_data:\n",
        "        legend_elements.append(Line2D([0], [0], color='grey', lw=4, marker=r'$\\rightarrow$', label='ocean current', linestyle=''))\n",
        "    if plot_wind_data:\n",
        "        legend_elements.append(Line2D([0], [0], color='black', lw=4, marker=r'$\\rightarrow$', label='wind direction', linestyle=''))\n",
        "\n",
        "\n",
        "    # Legende außerhalb der Plots, neben der Colorbar\n",
        "    fig.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(0.13, 0.05), frameon=True, title=\"\")\n",
        "\n",
        "\n",
        "\n",
        "    # fig.suptitle(title, \n",
        "    #     #fontsize=16, \n",
        "    #     y=0.99)\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "        # Save the figure\n",
        "    if save:\n",
        "        fig_name = f\"{title.replace(' ', '_').replace(':', '')}.png\"\n",
        "        fig.savefig(f'../data/plots/{fig_name}', bbox_inches='tight', dpi=300)\n",
        "        print(f\"Figure saved as {fig_name}\")\n",
        "        \n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_for_timepoint(timepoint=\"2023-01-01 00:00:00\", ax=None, grid_size_ocean=50, wind_grid_size=20, vmin=-1.0, vmax=1.5, plot_water_velocity_data=True, plot_wind_data=True, cluster=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_size_ocean = 200\n",
        "wind_grid_size = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ocean.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_weather.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_insitu['time'].min(), df_insitu['time'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sturm_surge_dict = {\n",
        "    1: {\n",
        "        \n",
        "        \"timepoints\": sorted([\n",
        "                pd.Timestamp(\"2023-02-24 05:00\"),\n",
        "                pd.Timestamp(\"2023-02-24 15:00\"),\n",
        "                pd.Timestamp(\"2023-02-25 16:00\"),\n",
        "                pd.Timestamp(\"2023-02-26 15:00\"),\n",
        "        ])\n",
        "    },    \n",
        "    2: {\n",
        "        \"timepoints\": sorted([\n",
        "                pd.Timestamp(\"2023-03-30 21:00\"),\n",
        "                pd.Timestamp(\"2023-03-31 12:00\"),\n",
        "                pd.Timestamp(\"2023-04-01 12:00\"),\n",
        "                pd.Timestamp(\"2023-04-03 00:00\"),\n",
        "        ])\n",
        "    },\n",
        "    3: {\n",
        "        \"timepoints\": sorted([\n",
        "                pd.Timestamp(\"2023-10-06 00:00\"),\n",
        "                pd.Timestamp(\"2023-10-06 22:00\"),\n",
        "                pd.Timestamp(\"2023-10-07 19:00\"),\n",
        "                pd.Timestamp(\"2023-10-09 07:00\"),\n",
        "        ])\n",
        "    },\n",
        "\n",
        "    4: {\n",
        "        \"timepoints\": sorted([\n",
        "                pd.Timestamp(\"2023-10-17 22:00\"), \n",
        "                pd.Timestamp(\"2023-10-19 21:00\"), \n",
        "                pd.Timestamp(\"2023-10-20 21:00\"),\n",
        "                pd.Timestamp(\"2023-10-21 17:00\"),   \n",
        "        ])\n",
        "    },\n",
        "    5: {\n",
        "        \"timepoints\": sorted([\n",
        "                pd.Timestamp(\"2024-01-02 05:00\"), \n",
        "                pd.Timestamp(\"2024-01-03 01:00\"),             \n",
        "                pd.Timestamp(\"2024-01-04 12:00\"), \n",
        "                pd.Timestamp(\"2024-01-05 05:00\"), \n",
        "                \n",
        "                \n",
        "        ])\n",
        "    },\n",
        "    6: {\n",
        "        \"timepoints\": sorted([\n",
        "                pd.Timestamp(\"2024-02-08 18:00\"), \n",
        "                pd.Timestamp(\"2024-02-09 15:00\"),         \n",
        "                pd.Timestamp(\"2024-02-11 13:00\"), \n",
        "                pd.Timestamp(\"2024-02-13 11:00\"), \n",
        "                \n",
        "                 \n",
        "        ])\n",
        "    },\n",
        "\n",
        "\n",
        "    7: {\n",
        "        \"timepoints\": sorted([\n",
        "                pd.Timestamp(\"2024-12-07 17:00\"),\n",
        "                pd.Timestamp(\"2024-12-09 05:00\"),\n",
        "                pd.Timestamp(\"2024-12-09 12:00\"),\n",
        "                pd.Timestamp(\"2024-12-10 17:00\"),\n",
        "        ])\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyse von 2023-10-19 bis 2023-10-21\n",
        "# timepoints = sorted([\n",
        "#     pd.Timestamp(\"2023-10-17 22:00\"), # links oben\n",
        "#     pd.Timestamp(\"2023-10-19 21:00\"), # links unten\n",
        "#     pd.Timestamp(\"2023-10-20 21:00\"), # rechts oben\n",
        "#     pd.Timestamp(\"2023-10-21 17:00\"), # rechts unten    \n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "# sub_box = {\n",
        "#         \"lat_min\": closest_point[\"latitude\"] - 0.1, # 54.4\n",
        "#         \"lat_max\": closest_point[\"latitude\"] + 0.1, # 55.5\n",
        "#         \"lon_min\": closest_point[\"longitude\"] - 0.1,\n",
        "#         \"lon_max\": closest_point[\"longitude\"] + 0.1 # 10.5\n",
        "#         }\n",
        "for number in sturm_surge_dict.keys():\n",
        "    \n",
        "    print(f\"\\n ################# Analyse Sturmflut {number} ################# \\n\")\n",
        "    timepoints = sturm_surge_dict[number][\"timepoints\"]\n",
        "    # plot_closest_location(df_ocean, sub_box=sub_box, target_lat=LAT_FLENSBURG, target_lon=LON_FLENSBURG)\n",
        "    \n",
        "    lineplot_storm_surge(df=df_insitu, column_name='slev', timepoints=timepoints, ax=None, show_legend=True, save=True, legend_outside=False)\n",
        "\n",
        "    analyse_storm_surges(\n",
        "        title=f\"Storm surge analysis from {timepoints[0].strftime('%Y-%m-%d')} to {timepoints[-1].strftime('%Y-%m-%d')}\", \n",
        "        timepoints=timepoints, \n",
        "        grid_size_ocean=grid_size_ocean, \n",
        "        wind_grid_size=wind_grid_size,\n",
        "        plot_water_velocity_data=True,\n",
        "        plot_wind_data=True,\n",
        "        save=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_features(column_name:str, sturm_surge_dict:dict, calculation:str):\n",
        "    \"\"\"\n",
        "    Compare features of the ocean data and weather data for a given timepoint.\n",
        "    \"\"\"\n",
        "    keys = list(sturm_surge_dict.keys())\n",
        "\n",
        "\n",
        "    # Neue Anordnung: 3 Zeilen, 3 Spalten → Zeitreihe oben, darunter 3x3 Heatmaps\n",
        "    n_cols = 3  # Anzahl der Spalten\n",
        "    n_rows = (len(keys) + n_cols - 1) // n_cols  # Berechne die benötigten Zeilen\n",
        "    fig = plt.figure(figsize=(8*n_cols, 5*n_rows), constrained_layout=True)\n",
        "    gs = gridspec.GridSpec(n_rows, n_cols, \n",
        "                           #height_ratios=[0.4] + [0.8] * (n_rows - 1), \n",
        "                           hspace=0.7, wspace=0.2)\n",
        "\n",
        "\n",
        "    if column_name in INSITU_DICT:\n",
        "        df_plot = df_insitu.copy()\n",
        "    elif column_name in OCEAN_DICT:\n",
        "        df_plot = df_ocean.copy()\n",
        "    elif column_name in WEATHER_DICT:\n",
        "        df_plot = df_weather.copy()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "    timepoints = sturm_surge_dict[keys[0]][\"timepoints\"]\n",
        "    letter_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I'}\n",
        "\n",
        "    ax_lines = []\n",
        "    for i, key in enumerate(keys):\n",
        "        ax = fig.add_subplot(gs[i // n_cols, i % n_cols])  # Zeile und Spalte dynamisch berechnen\n",
        "\n",
        "        timepoints = sturm_surge_dict[key][\"timepoints\"]\n",
        "\n",
        "        ax_line = lineplot_storm_surge(df=df_plot, column_name=column_name, timepoints=timepoints,\n",
        "                                       ax=ax, calculation=calculation, show_number_lines=False, display=False,\n",
        "                                       title=f'Figure {letter_dict[i]}: {timepoints[0].strftime(\"%Y-%m-%d\")} - {timepoints[-1].strftime(\"%Y-%m-%d\")}',\n",
        "                                       display_sturm_surge=True)\n",
        "        ax_lines.append(ax_line)\n",
        "    \n",
        "    # set legend\n",
        "    ax_lines[2].legend(\n",
        "            #title=\"Beaufort Scale\",\n",
        "            loc=\"upper right\",\n",
        "            #fontsize=12,\n",
        "            #title_fontsize=10,\n",
        "            framealpha=0.0,\n",
        "            facecolor='white',\n",
        "            #edgecolor='gray',\n",
        "            ncol=1,\n",
        "            fancybox=True,\n",
        "            shadow=False,\n",
        "            #borderaxespad=0.3,\n",
        "            bbox_to_anchor=(1.5, 1),\n",
        "        )\n",
        "        \n",
        "    if column_name in INSITU_DICT:\n",
        "        title = f\"Observation of {INSITU_DICT[column_name]['description'].lower()} ({column_name}) for different storm surges\"\n",
        "    elif column_name in WEATHER_DICT:\n",
        "        title = f\"Mean value of {WEATHER_DICT[column_name]['description'].lower()} ({column_name}) in target box for different storm surges\\nDescription: {WEATHER_DICT[column_name]['explanation']}\"\n",
        "    elif column_name in OCEAN_DICT:\n",
        "        title = f\"Mean value of {OCEAN_DICT[column_name]['description'].lower()} ({column_name}) in target box for different storm surges\\nDescription: {OCEAN_DICT[column_name]['explanation']}\"\n",
        "        if column_name == 'sla':\n",
        "            title = f\"Mean value of {OCEAN_DICT[column_name]['description'].lower()} ({column_name}) in target box for different storm surges\"\n",
        "    else:\n",
        "        title = f\"Mean value of {column_name.lower()} in target box for different storm surges\"\n",
        "\n",
        "    fig.suptitle(f'{title}', \n",
        "        #fontsize=24, \n",
        "        y=0.99)\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "    # Save the figure\n",
        "    fig_name = f\"../data/plots/datacompare_{column_name}.png\"\n",
        "    fig.savefig(fig_name, bbox_inches='tight', dpi=300)\n",
        "    print(f\"Figure saved as {fig_name}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All Line Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_all = df_ocean.columns.tolist() + df_weather.columns.tolist() + df_insitu.columns.tolist()\n",
        "features_all = ['slev', 'sla', 'uo', 'vo', 'wo', 'wind_speed_10m', 'wind_direction_10m', 'pressure_msl']\n",
        "\n",
        "for feature in features_all:\n",
        "    compare_features(column_name=feature, sturm_surge_dict=sturm_surge_dict, calculation='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Korrelation zwischen SLEV und features on map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ocean = load_ocean_data(ocean_data_path, OCEAN_POINTS, verbose=False)\n",
        "df_ocean = process_df(df_ocean, drop_cols=[\"depth\"], verbose=False)\n",
        "\n",
        "df_weather = load_weather_data(weather_data_path, WEATHER_POINTS, verbose=False)\n",
        "df_weather = process_df(df_weather, verbose=False)\n",
        "\n",
        "df_insitu = load_insitu_data(verbose=False)\n",
        "df_insitu = process_flensburg_data(df_insitu, \n",
        "                                      start_time=df_ocean['time'].min(),\n",
        "                                      end_time=df_ocean['time'].max(),\n",
        "                                      verbose=False)\n",
        "\n",
        "df_insitu = group_data_hourly(df_insitu)\n",
        "df_insitu = process_df(df_insitu, drop_cols=[\"deph\"], verbose=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "resolution = 0.25 # 0.25 degrees\n",
        "\n",
        "def interpolate_xarray(ds, resolution=0.25, make_fine_grid=True, interpolate_nan=True):\n",
        "    \"\"\"\n",
        "    Interpolates the xarray dataset to a finer grid.\n",
        "\n",
        "    Parameters:\n",
        "        ds (xarray.Dataset): The input dataset to interpolate.\n",
        "        resolution (float): The desired resolution for the interpolation.\n",
        "\n",
        "    Returns:\n",
        "        xarray.Dataset: The interpolated dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    if interpolate_nan:\n",
        "        # Interpolation von NaN-Werten\n",
        "        ds = ds.interpolate_na(dim=\"time\", method=\"linear\")\n",
        "        ds = ds.interpolate_na(dim=\"latitude\", method=\"linear\")\n",
        "        ds = ds.interpolate_na(dim=\"longitude\", method=\"linear\")\n",
        "    \n",
        "    if make_fine_grid:\n",
        "        # Neues feineres Gitter erzeugen\n",
        "        new_lats = np.arange(ds.latitude.min(), ds.latitude.max(), resolution)\n",
        "        new_lons = np.arange(ds.longitude.min(), ds.longitude.max(), resolution)\n",
        "\n",
        "        # Interpolation\n",
        "        ds = ds.interp(latitude=new_lats, longitude=new_lons, method=\"linear\")\n",
        "  \n",
        "    return ds\n",
        "\n",
        "# Create xarray datasets from DataFrames\n",
        "# Interpolate the xarray to a higher resolution\n",
        "ds_ocean = df_ocean.set_index([\"time\", \"latitude\", \"longitude\"]).to_xarray()\n",
        "ds_ocean_interp = interpolate_xarray(ds_ocean, resolution=resolution, make_fine_grid=True, interpolate_nan=True)\n",
        "\n",
        "ds_weather = df_weather.set_index([\"time\", \"latitude\", \"longitude\"]).to_xarray()\n",
        "ds_weather_interp = interpolate_xarray(ds_weather, resolution=resolution, make_fine_grid=True, interpolate_nan=True)\n",
        "\n",
        "# Aline the time axes\n",
        "common_time = np.intersect1d(ds_ocean_interp.time.values, ds_weather_interp.time.values) # Finde common time points\n",
        "ds_ocean_interp = ds_ocean_interp.sel(time=common_time)\n",
        "ds_weather_interp = ds_weather_interp.sel(time=common_time)\n",
        "\n",
        "# Aline the lat and lon axes\n",
        "ds_weather_interp = ds_weather_interp.interp(\n",
        "    latitude=ds_ocean_interp.latitude,\n",
        "    longitude=ds_ocean_interp.longitude\n",
        ")\n",
        "\n",
        "ds_weather_interp = interpolate_xarray(ds_weather_interp, resolution=resolution, make_fine_grid=False, interpolate_nan=True)\n",
        "ds_ocean_interp = interpolate_xarray(ds_ocean_interp, resolution=resolution, make_fine_grid=False, interpolate_nan=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_ocean_mask(ds, land):\n",
        "\n",
        "    lon, lat = np.meshgrid(ds.longitude.values, ds.latitude.values)\n",
        "    points = [shapely.geometry.Point(x, y) for x, y in zip(lon.flatten(), lat.flatten())]\n",
        "\n",
        "    points_gdf = gpd.GeoDataFrame(geometry=points, crs=land.crs)\n",
        "    joined = gpd.sjoin(points_gdf, land, predicate=\"within\", how=\"left\")\n",
        "    on_land = ~joined.index_right.isna()\n",
        "\n",
        "    mask_land = np.array(on_land).reshape(lat.shape)\n",
        "    mask_ocean = ~mask_land\n",
        "\n",
        "    ocean_mask_xr = xr.DataArray(\n",
        "        mask_ocean,\n",
        "        coords={\"latitude\": ds.latitude, \"longitude\": ds.longitude},\n",
        "        dims=[\"latitude\", \"longitude\"]\n",
        "    )\n",
        "    return ocean_mask_xr\n",
        "\n",
        "land = gpd.read_file(geodatasets.get_path(\"naturalearth.land\"))\n",
        "ocean_mask = create_ocean_mask(ds_weather_interp, land)\n",
        "ds_weather_ocean_only = ds_weather_interp.where(ocean_mask)\n",
        "ds_ocean_ocean_only = ds_ocean_interp.where(ocean_mask)\n",
        "\n",
        "ds_ocean_weather_interp = xr.merge([ds_ocean_ocean_only, ds_weather_interp])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_ocean_weather_interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def calculate_correlation_temporal_spatial(ds, df_ref, variable='sla', start_date=None, end_date=None, ax=None, title=None):\n",
        "    \"\"\"\n",
        "    Berechnet die Korrelation zwischen einer xarray-Zeitreihe und einer Pandas-Zeitreihe.\n",
        "    \n",
        "    Args:\n",
        "        ds_ocean (xarray.Dataset): Das xarray-Dataset mit den Ozeandaten.\n",
        "        df_ref (pandas.DataFrame): Das DataFrame mit den In-situ-Daten.\n",
        "        variable (str): Der Name der Variablen im xarray-Dataset, die korreliert werden soll.\n",
        "        \n",
        "    Returns:\n",
        "        correlations (numpy.ndarray): Ein Array mit den Korrelationswerten.\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    fig = None\n",
        "\n",
        "    ocean_variables = ['bottomT', 'mlotst', 'siconc', 'sithick', \n",
        "                       'sla', 'so', 'sob', 'thetao', 'uo', 'vo', 'wo']\n",
        "    \n",
        "    air_variables = ['temperature_2m',\n",
        "                    'relative_humidity_2m', 'dew_point_2m', 'apparent_temperature',\n",
        "                    'precipitation', 'rain', 'showers', 'snowfall', 'weather_code',\n",
        "                    'pressure_msl', 'surface_pressure', 'cloud_cover', 'cloud_cover_low',\n",
        "                    'cloud_cover_mid', 'cloud_cover_high', 'et0_fao_evapotranspiration',\n",
        "                    'vapour_pressure_deficit', 'wind_speed_10m', 'wind_direction_10m',\n",
        "                    'wind_gusts_10m']\n",
        "    \n",
        "\n",
        "    df_ref_filtered = df_ref.copy()\n",
        "    ds_filtered = ds.copy()\n",
        "\n",
        "    if start_date is not None and end_date is not None:\n",
        "        # Filtere die Daten nach dem angegebenen Zeitraum\n",
        "        df_ref_filtered = df_ref_filtered[(df_ref_filtered['time'] >= start_date) & (df_ref_filtered['time'] <= end_date)]\n",
        "        # Filtere die xarray-Daten nach dem angegebenen Zeitraum\n",
        "        ds_filtered = ds_filtered.sel(time=slice(start_date, end_date))\n",
        "    else:\n",
        "        # Wenn kein Zeitraum angegeben ist, verwende die gesamte Zeitreihe\n",
        "        start_date = ds_filtered['time'].min().values\n",
        "        end_date = ds_filtered['time'].max().values\n",
        "\n",
        "        # Filtere die xarray-Daten nach dem gesamten Zeitraum\n",
        "    \n",
        "    # slev-Zeitreihe (von df_ref_filtered)\n",
        "    slev_times = pd.to_datetime(df_ref_filtered['time'])\n",
        "    slev_values = df_ref_filtered['slev'].values\n",
        "\n",
        "    # Sicherstellen, dass die Zeiten übereinstimmen\n",
        "    # Zeitstempel vom xarray\n",
        "    ocean_times = pd.to_datetime(ds_filtered['time'].values)\n",
        "\n",
        "    # Indexe der gemeinsamen Zeiten\n",
        "    common_times, idx_slev, idx_ocean = np.intersect1d(slev_times, ocean_times, return_indices=True)\n",
        "\n",
        "    # neue Zeitreihen\n",
        "    slev_values_common = slev_values[idx_slev]\n",
        "    ocean_times_common = ocean_times[idx_ocean]\n",
        "\n",
        "    # Leeres Array für Korrelationen\n",
        "    correlations = np.full((len(ds_filtered.latitude), len(ds_filtered.longitude)), np.nan)\n",
        "\n",
        "    # Schleife über alle Punkte\n",
        "    for i, lat in enumerate(ds_filtered.latitude.values):\n",
        "        for j, lon in enumerate(ds_filtered.longitude.values):\n",
        "            # Zeitreihe an diesem Punkt\n",
        "            ts = ds_filtered[variable].isel(latitude=i, longitude=j).values\n",
        "            \n",
        "            if np.all(np.isnan(ts)):  # Wenn nur NaNs -> überspringen\n",
        "                continue\n",
        "            \n",
        "            # nur gemeinsame Zeiten auswählen\n",
        "            ts_common = ts[idx_ocean]\n",
        "            \n",
        "            # Wenn zu viele NaNs, überspringen\n",
        "            if np.isnan(ts_common).mean() > 0.3:  # z.B. >30% fehlende Werte\n",
        "                continue\n",
        "            \n",
        "            # NaNs behandeln\n",
        "            mask = ~np.isnan(ts_common) & ~np.isnan(slev_values_common)\n",
        "            if np.sum(mask) < 10:  # Weniger als 10 gültige Werte\n",
        "                continue\n",
        "            \n",
        "            # Korrelation berechnen\n",
        "            corr = np.corrcoef(ts_common[mask], slev_values_common[mask])[0, 1]\n",
        "            correlations[i, j] = corr\n",
        "\n",
        "    # Plotten der Korrelationen\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(14, 12))\n",
        "        created_fig = True\n",
        "    else:\n",
        "        created_fig = False\n",
        "\n",
        "\n",
        "    m = Basemap(projection='cyl', resolution='i',\n",
        "                llcrnrlon=ds_filtered.longitude.min(), urcrnrlon=ds_filtered.longitude.max(),\n",
        "                llcrnrlat=ds_filtered.latitude.min(), urcrnrlat=ds_filtered.latitude.max(), ax=ax)\n",
        "\n",
        "    if variable in air_variables:\n",
        "        m.drawcoastlines(ax=ax)\n",
        "        m.drawcountries(ax=ax)\n",
        "        m.drawmapboundary(fill_color='white', ax=ax)\n",
        "        m.fillcontinents(color='lightgrey', lake_color='white', ax=ax)\n",
        "        heatmap = ax.pcolormesh(ds_filtered.longitude, ds_filtered.latitude, correlations, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "        plt.colorbar(heatmap, label='Pearson correlation coefficient', orientation='horizontal', pad=0.1, ax=ax, shrink=0.9)\n",
        "        ax.scatter(df_ref_filtered['longitude'], df_ref_filtered['latitude'], c='green', s=200, label='Flensburg', marker='*')\n",
        "\n",
        "    m.drawparallels(np.arange(-360., 360, 2.), labels=[1, 0, 0, 0])\n",
        "    m.drawmeridians(np.arange(-360., 360, 2.), labels=[0, 0, 0, 1])    \n",
        "\n",
        "    if variable in ocean_variables:\n",
        "        heatmap = ax.pcolormesh(ds_filtered.longitude, ds_filtered.latitude, correlations, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "        plt.colorbar(heatmap, label='Pearson correlation coefficient', orientation='horizontal', pad=0.1, ax=ax, shrink=0.9)\n",
        "        m.drawcoastlines(ax=ax)\n",
        "        m.drawcountries(ax=ax)\n",
        "        m.drawmapboundary(fill_color='white', ax=ax)\n",
        "        m.fillcontinents(color='lightgrey', lake_color='white', ax=ax)\n",
        "        ax.scatter(df_ref_filtered['longitude'], df_ref_filtered['latitude'], c='green', s=250, label='Flensburg', marker='*') \n",
        "    \n",
        "    # Set title to describe the variable\n",
        "    if variable in WEATHER_DICT.keys():\n",
        "        var_name = WEATHER_DICT[variable]['description'].lower()\n",
        "    if variable in OCEAN_DICT.keys():\n",
        "        var_name = OCEAN_DICT[variable]['description'].lower()\n",
        "    else:\n",
        "        var_name = variable\n",
        "    \n",
        "    start_date_str = pd.to_datetime(start_date).strftime('%Y-%m-%d %H:%M')\n",
        "    end_date_str = pd.to_datetime(end_date).strftime('%Y-%m-%d %H:%M')\n",
        "    if title is not None:\n",
        "\n",
        "        plt.title(title, pad=15)\n",
        "    else:\n",
        "        plt.title(f'{start_date_str} to {end_date_str}', pad=15)\n",
        "        \n",
        "    plt.legend(loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if created_fig:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# variables = ['sla', 'uo', 'vo', 'wind_speed_10m', 'wind_direction_10m', 'surface_pressure']\n",
        "# #variables = ds_ocean_weather_interp.data_vars.keys()\n",
        "# #variables = ['wind_speed_10m']\n",
        "\n",
        "# for variable in variables:\n",
        "#     print(f\"Calculating correlation for {variable}\")\n",
        "#     calculate_correlation_temporal_spatial(ds_ocean_weather_interp, df_insitu, variable=variable, \n",
        "#                                            #start_date=\"2023-10-19\", end_date=\"2023-10-21\"\n",
        "#                                            )\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# start_date = ds_ocean_weather_interp['time'].min().values\n",
        "# end_date = ds_ocean_weather_interp['time'].max().values\n",
        "\n",
        "# # round datetime to YYYY-MM-DD HH:MM\n",
        "# start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d %H:%M')\n",
        "# end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "# variables = ['sla', 'uo', 'vo', 'wind_speed_10m', 'wind_direction_10m', 'surface_pressure']\n",
        "# variables\n",
        "\n",
        "def plot_correlations(variables, start_date, end_date, title:str = None, subplot_title:bool = False,save:bool = False, one_plot:bool = False):\n",
        "    \"\"\"\n",
        "    Plots the correlations between ocean variables and Flensburg SLEV.\n",
        "    \n",
        "    Args:\n",
        "        variables (list): List of variable names to plot.\n",
        "        start_date (str): Start date for the analysis.\n",
        "        end_date (str): End date for the analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    if one_plot:\n",
        "        n_cols = 3\n",
        "        n_rows = (len(variables) + n_cols - 1) // n_cols\n",
        "\n",
        "        fig = plt.figure(figsize=(8 * n_cols, 5 * n_rows))\n",
        "        gs = gridspec.GridSpec(n_rows, n_cols, figure=fig)\n",
        "\n",
        "        if title is not None:\n",
        "            fig.suptitle(title, \n",
        "                        #fontsize=20, \n",
        "                        y=0.98\n",
        "                        )\n",
        "        else:\n",
        "            title = f\"Correlation between Ocean Variables and Flensburg SLEV from {start_date} to {end_date}\"\n",
        "            fig.suptitle(title,\n",
        "                            #fontsize=20,\n",
        "                            y=0.98\n",
        "                            )\n",
        "            \n",
        "        \n",
        "\n",
        "\n",
        "        letter_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I'}\n",
        "        \n",
        "\n",
        "        for idx, variable in enumerate(variables):\n",
        "\n",
        "            print(f\"Calculating correlation for {variable}\")\n",
        "            ax = fig.add_subplot(gs[idx])\n",
        "            calculate_correlation_temporal_spatial(\n",
        "                ds_ocean_weather_interp,\n",
        "                df_insitu,\n",
        "                variable=variable,\n",
        "                ax=ax,\n",
        "                start_date=start_date,\n",
        "                end_date=end_date,\n",
        "                title=f'Figure {letter_dict[idx]}: {variable}',\n",
        "            )\n",
        "\n",
        "        # if subplot_title:\n",
        "        #     for ax, variable in zip(fig.axes, variables):\n",
        "        #         if variable in WEATHER_DICT.keys():\n",
        "        #             var_name = WEATHER_DICT[variable]['description'].lower()\n",
        "        #         elif variable in OCEAN_DICT.keys():\n",
        "        #             var_name = OCEAN_DICT[variable]['description'].lower()\n",
        "        #         else:\n",
        "        #             var_name = variable\n",
        "        #         ax.set_title(f'Variable {variable} and slev', pad=15)\n",
        "\n",
        "        # Statt tight_layout --> subplots_adjust\n",
        "        fig.subplots_adjust(top=0.90, hspace=0.2, wspace=0.1)  # <-- manuell fein justieren!\n",
        "        if save:\n",
        "            fig_name = '../data/plots/correlation_all.png'\n",
        "            fig.savefig(fig_name, bbox_inches='tight', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        for idx, variable in enumerate(variables):\n",
        "            print(f\"Calculating correlation for {variable}\")\n",
        "            fig, ax = plt.subplots(figsize=(14, 12))\n",
        "            calculate_correlation_temporal_spatial(\n",
        "                ds_ocean_weather_interp,\n",
        "                df_insitu,\n",
        "                variable=variable,\n",
        "                ax=ax,\n",
        "                start_date=start_date,\n",
        "                end_date=end_date,\n",
        "            )\n",
        "            if title is not None:\n",
        "                plt.title(title, pad=15)\n",
        "            else:\n",
        "                plt.title(f'Correlation between {variable} and Flensburg SLEV from {start_date} to {end_date}', pad=15)\n",
        "            plt.tight_layout()\n",
        "            if save:\n",
        "                fig_name = f'../thesis_plots/all_correlation_{variable}_map.png'\n",
        "                fig.savefig(fig_name, bbox_inches='tight', dpi=300)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def compare_correlations(timepoints:list, variable:str,save:bool = False):\n",
        "    \n",
        "    n_cols = 3\n",
        "    n_rows = (len(timepoints) + n_cols -1) // n_cols\n",
        "    fig = plt.figure(figsize=(8 * n_cols, 5 * n_rows), dpi=300)\n",
        "\n",
        "    #fig = plt.figure(figsize=(13.33, 7.5))\n",
        "    gs = gridspec.GridSpec(n_rows, n_cols, figure=fig)\n",
        "\n",
        "    if variable in WEATHER_DICT.keys():\n",
        "        var_name = WEATHER_DICT[variable]['description'].lower()\n",
        "        var_explaination = WEATHER_DICT[variable]['explanation']\n",
        "        \n",
        "        #var_unit = WEATHER_DICT[variable]['unit']\n",
        "    elif variable in OCEAN_DICT.keys():\n",
        "        var_name = OCEAN_DICT[variable]['description'].lower()\n",
        "        var_explaination = OCEAN_DICT[variable]['explanation']\n",
        "        #var_unit = WEATHER_DICT[variable]['unit']\n",
        "    else:\n",
        "        var_name = variable\n",
        "        var_explaination = \"\"\n",
        "        \n",
        "        # return None\n",
        "        #var_unit = \"\"\n",
        "    fig.suptitle(f\"Correlation between water level in Flensburg (slev) and {var_name} ({variable})\", \n",
        "                 #fontsize=20, \n",
        "                 y=0.98)\n",
        "    \n",
        "\n",
        "    for idx, timepoint in tqdm(enumerate(timepoints), total=len(timepoints)):\n",
        "\n",
        "        letter_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I'}\n",
        "\n",
        "        start_date = timepoint - pd.Timedelta(days=3)\n",
        "        end_date = timepoint + pd.Timedelta(days=3)\n",
        "        ax = fig.add_subplot(gs[idx])\n",
        "        start_date_str = pd.to_datetime(start_date).strftime('%Y-%m-%d %H:%M')\n",
        "        end_date_str = pd.to_datetime(end_date).strftime('%Y-%m-%d %H:%M')\n",
        "        calculate_correlation_temporal_spatial(\n",
        "            ds_ocean_weather_interp,\n",
        "            df_insitu,\n",
        "            variable=variable,\n",
        "            ax=ax,\n",
        "            start_date=start_date,\n",
        "            end_date=end_date,\n",
        "            title=f\"Figure {letter_dict[idx]}: {start_date_str} - {end_date_str}\",\n",
        "        \n",
        "        )\n",
        "    fig.subplots_adjust(top=0.92, hspace=0.25, wspace=0.1)  # <-- manuell fein justieren!\n",
        "\n",
        "    \n",
        "    if save:\n",
        "        fig_name = f'../data/plots/correlation_{variable}.png'\n",
        "        fig.savefig(fig_name, bbox_inches='tight', dpi=300)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "timepoints_all = sorted(set(df_ocean[\"time\"]) & set(df_weather[\"time\"]))\n",
        "\n",
        "sturm_surge_list = sorted([\n",
        "                datetime.datetime(2023, 2, 25, 17, 0),\n",
        "                datetime.datetime(2023, 4, 1, 12, 0),\n",
        "                datetime.datetime(2023, 10, 7, 20, 0),\n",
        "                datetime.datetime(2023, 10, 20, 0, 0),\n",
        "                datetime.datetime(2024, 1, 3, 9, 0),\n",
        "                datetime.datetime(2024, 2, 9, 18, 0),\n",
        "                datetime.datetime(2024, 12, 9, 16, 0),\n",
        "                ])\n",
        "variables = ds_ocean_weather_interp.data_vars.keys()\n",
        "variables = [\"sla\", \"uo\", \"vo\", \"wind_speed_10m\", \"wind_direction_10m\", \"pressure_msl\"]\n",
        "for variable in variables:\n",
        "    print(f\"Calculating correlation for {variable}\")\n",
        "    compare_correlations(timepoints=sturm_surge_list, variable=variable, save=False, )\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_ocean_weather_interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_date = ds_ocean_weather_interp['time'].min().values\n",
        "end_date = ds_ocean_weather_interp['time'].max().values\n",
        "\n",
        "# round datetime to YYYY-MM-DD HH:MM\n",
        "start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d %H:%M')\n",
        "end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "print(f\"Start date: {start_date}, End date: {end_date}\")\n",
        "\n",
        "variables = ds_ocean_weather_interp.data_vars.keys()\n",
        "variables = [\"sla\", \"uo\", \"vo\", \"wind_speed_10m\", \"wind_direction_10m\", \"pressure_msl\"]\n",
        "\n",
        "plot_correlations(variables, start_date, end_date,\n",
        "                  title=\" \",\n",
        "                  subplot_title=True,\n",
        "                  save=True,\n",
        "                  one_plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "start_date = datetime.datetime(2023, 10, 17, 0, 0).strftime(\"%Y-%m-%d %H:%M\")\n",
        "end_date = datetime.datetime(2023, 10, 23, 0, 0).strftime(\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "variables = ds_ocean_weather_interp.data_vars.keys()\n",
        "\n",
        "plot_water_level_anomalies(df_insitu, start_date=start_date, end_date=end_date)\n",
        "plot_correlations(\n",
        "    variables,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "sturm_surge_list = [datetime.datetime(2023, 2, 25, 17, 0),\n",
        "                    datetime.datetime(2023, 4, 1, 12, 0),\n",
        "                    datetime.datetime(2023, 10, 7, 20, 0),\n",
        "                    datetime.datetime(2023, 10, 20, 0, 0),\n",
        "                    datetime.datetime(2024, 2, 9, 18, 0),\n",
        "                    datetime.datetime(2024, 12, 9, 16, 0),\n",
        "                    ]\n",
        "\n",
        "for time in sturm_surge_list:\n",
        "    start_time = time - datetime.timedelta(days=1)\n",
        "    end_time = time + datetime.timedelta(days=1)\n",
        "    df_insitu_sturm = df_insitu[(df_insitu[\"time\"] >= start_time) & (df_insitu[\"time\"] <= end_time)]\n",
        "    plot_water_level_anomalies(df_insitu_sturm, start_date=start_date, end_date=end_date)\n",
        "    plot_correlations(\n",
        "        variables,\n",
        "        start_date=start_time,\n",
        "        end_date=end_time\n",
        "    )\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Correlaion just at sturm surges \n",
        "\n",
        "df_surge = df_insitu.copy()\n",
        "df_surge = df_surge.loc[df_surge['slev'] >= 0.5]\n",
        "start_date = df_surge['time'].min()\n",
        "end_date = df_surge['time'].max()\n",
        "# round datetime to YYYY-MM-DD HH:MM\n",
        "start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d %H:%M')\n",
        "end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d %H:%M')\n",
        "variables = ds_ocean_weather_interp.data_vars.keys()\n",
        "plot_water_level_anomalies(df_surge, start_date=start_date, end_date=end_date)\n",
        "plot_correlations(\n",
        "    variables,\n",
        "    start_date=start_date,\n",
        "    end_date=end_date\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_weather.showers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lassen sich Korrelationen zwischen den Features und dem Wasserpegel (sla) erkennen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ocean = load_ocean_data(ocean_data_path, OCEAN_POINTS, verbose=False)\n",
        "df_ocean = process_df(df_ocean, drop_cols=[\"depth\"], verbose=False)\n",
        "\n",
        "df_weather = load_weather_data(weather_data_path, WEATHER_POINTS, verbose=False)\n",
        "df_weather = process_df(df_weather, verbose=False)\n",
        "\n",
        "df_insitu = load_insitu_data(verbose=False)\n",
        "df_insitu = process_flensburg_data(df_insitu, \n",
        "                                      start_time=df_ocean['time'].min(),\n",
        "                                      end_time=df_ocean['time'].max(),\n",
        "                                      verbose=False)\n",
        "\n",
        "df_insitu = group_data_hourly(df_insitu)\n",
        "df_insitu = process_df(df_insitu, drop_cols=[\"deph\"], verbose=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geodatasets\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "resolution = 0.25 # 0.25 degrees\n",
        "\n",
        "def interpolate_xarray(ds, resolution=0.25, make_fine_grid=True, interpolate_nan=True):\n",
        "    \"\"\"\n",
        "    Interpolates the xarray dataset to a finer grid.\n",
        "\n",
        "    Parameters:\n",
        "        ds (xarray.Dataset): The input dataset to interpolate.\n",
        "        resolution (float): The desired resolution for the interpolation.\n",
        "\n",
        "    Returns:\n",
        "        xarray.Dataset: The interpolated dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    if interpolate_nan:\n",
        "        # Interpolation von NaN-Werten\n",
        "        ds = ds.interpolate_na(dim=\"time\", method=\"linear\")\n",
        "        ds = ds.interpolate_na(dim=\"latitude\", method=\"linear\")\n",
        "        ds = ds.interpolate_na(dim=\"longitude\", method=\"linear\")\n",
        "    \n",
        "    if make_fine_grid:\n",
        "        # Neues feineres Gitter erzeugen\n",
        "        new_lats = np.arange(ds.latitude.min(), ds.latitude.max(), resolution)\n",
        "        new_lons = np.arange(ds.longitude.min(), ds.longitude.max(), resolution)\n",
        "\n",
        "        # Interpolation\n",
        "        ds = ds.interp(latitude=new_lats, longitude=new_lons, method=\"linear\")\n",
        "  \n",
        "    return ds\n",
        "\n",
        "# Create xarray datasets from DataFrames\n",
        "# Interpolate the xarray to a higher resolution\n",
        "ds_ocean = df_ocean.set_index([\"time\", \"latitude\", \"longitude\"]).to_xarray()\n",
        "ds_ocean_interp = interpolate_xarray(ds_ocean, resolution=resolution, make_fine_grid=True, interpolate_nan=True)\n",
        "\n",
        "ds_weather = df_weather.set_index([\"time\", \"latitude\", \"longitude\"]).to_xarray()\n",
        "ds_weather_interp = interpolate_xarray(ds_weather, resolution=resolution, make_fine_grid=True, interpolate_nan=True)\n",
        "\n",
        "# Aline the time axes\n",
        "common_time = np.intersect1d(ds_ocean_interp.time.values, ds_weather_interp.time.values) # Finde common time points\n",
        "ds_ocean_interp = ds_ocean_interp.sel(time=common_time)\n",
        "ds_weather_interp = ds_weather_interp.sel(time=common_time)\n",
        "\n",
        "# Aline the lat and lon axes\n",
        "ds_weather_interp = ds_weather_interp.interp(\n",
        "    latitude=ds_ocean_interp.latitude,\n",
        "    longitude=ds_ocean_interp.longitude\n",
        ")\n",
        "\n",
        "ds_weather_interp = interpolate_xarray(ds_weather_interp, resolution=resolution, make_fine_grid=False, interpolate_nan=True)\n",
        "ds_ocean_interp = interpolate_xarray(ds_ocean_interp, resolution=resolution, make_fine_grid=False, interpolate_nan=True)\n",
        "\n",
        "def create_ocean_mask(ds, land):\n",
        "\n",
        "    lon, lat = np.meshgrid(ds.longitude.values, ds.latitude.values)\n",
        "    points = [shapely.geometry.Point(x, y) for x, y in zip(lon.flatten(), lat.flatten())]\n",
        "\n",
        "    points_gdf = gpd.GeoDataFrame(geometry=points, crs=land.crs)\n",
        "    joined = gpd.sjoin(points_gdf, land, predicate=\"within\", how=\"left\")\n",
        "    on_land = ~joined.index_right.isna()\n",
        "\n",
        "    mask_land = np.array(on_land).reshape(lat.shape)\n",
        "    mask_ocean = ~mask_land\n",
        "\n",
        "    ocean_mask_xr = xr.DataArray(\n",
        "        mask_ocean,\n",
        "        coords={\"latitude\": ds.latitude, \"longitude\": ds.longitude},\n",
        "        dims=[\"latitude\", \"longitude\"]\n",
        "    )\n",
        "    return ocean_mask_xr\n",
        "\n",
        "land = gpd.read_file(geodatasets.get_path(\"naturalearth.land\"))\n",
        "ocean_mask = create_ocean_mask(ds_weather_interp, land)\n",
        "ds_weather_ocean_only = ds_weather_interp.where(ocean_mask)\n",
        "ds_ocean_ocean_only = ds_ocean_interp.where(ocean_mask)\n",
        "\n",
        "ds_ocean_weather_interp = xr.merge([ds_ocean_ocean_only, ds_weather_interp])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Calculating Bivariate Moran’s I "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Erklärung:** \n",
        "\n",
        "**Abgrenzung zu Korrelation und zeitlicher Autokorrelation**\n",
        "Bei der Berechnung der statistischen Korrelation werden zwei Variablen (x,y) bei zwei oder mehr Beobachtungen betrachtet; bei der räumlichen Autokorrelation hingegen eine Variable x an zwei oder mehr Orten.[3]\n",
        "\n",
        "Während die zeitliche Autokorrelation die Beziehungen der Ausprägungen einer Variablen mit sich selbst über die Zeit beschreibt, beschreibt die räumliche Autokorrelation die Ausprägungen einer Variablen mit sich selbst im Raum. \n",
        "\n",
        "**Berechnung**\n",
        "*Positive räumliche Autokorrelation *liegt dann vor, wenn nahe beieinander liegende Orte einander mit höherer Wahrscheinlichkeit ähnlich sind als weiter voneinander entfernte Orte. Das heißt: Positive räumliche Autokorrelation liegt vor, wenn Orte dazu tendieren, im Hinblick auf eine Eigenschaft Cluster zu bilden. Positive räumliche Autokorrelation ist eine empirische Manifestation von [Toblers](https://de.wikipedia.org/wiki/Erstes_Gesetz_der_Geographie) „Erstem Gesetz der Geographie“.\n",
        "\n",
        "*Negative räumliche Autokorrelation* liegt dann vor, wenn benachbarte Orte im Vergleich zu zufälliger Anordnung[5] unterschiedliche Eigenschaftswerte aufweisen. Bei Phänomenen, die mit Lebewesen (Tieren, Pflanzen) verbunden sind, wird negative Autokorrelation häufig durch Wettbewerb und Verdrängung verursacht.\n",
        "d\n",
        "*Keine räumliche Autokorrelation* liegt vor, wenn die Orte im Hinblick auf eine Eigenschaft zufällig angeordnet sind, also keine ausgeprägten Cluster aufweisen.\n",
        "\n",
        "[Wikipedia](https://de.wikipedia.org/wiki/Räumliche_Autokorrelation?utm_source=chatgpt.com) \n",
        "\n",
        "**Was ist räumliche Autokorrelation (spatial autocorrelatio**n)?\n",
        "\n",
        "Räumliche Autokorrelation beschreibt, wie stark der Wert einer Variablen an einem Ort mit den Werten derselben Variablen an anderen Orten zusammenhängt.\n",
        "\n",
        "**Vergleich mit klassischer Korrelation:**\n",
        "Normale (statistische) Korrelation:\n",
        "→ Fragt: \"Wie hängen zwei verschiedene Variablen miteinander zusammen?\"\n",
        "Beispiel: Wenn der Luftdruck steigt, sinkt vielleicht die Regenwahrscheinlichkeit.\n",
        "Räumliche Autokorrelation:\n",
        "→ Fragt: \"Wie hängt der Wert einer einzigen Variable an einem Ort mit den Werten derselben Variable an benachbarten Orten zusammen?\"\n",
        "Beispiel: Ist die Wasserhöhe an Punkt A ähnlich wie an den umliegenden Punkten?\n",
        "\n",
        "**Und wie unterscheidet sich das von zeitlicher Autokorrelation?**\n",
        "Zeitliche Autokorrelation:\n",
        "→ Fragt: \"Wie hängt ein Wert heute mit den Werten derselben Variablen in der Vergangenheit oder Zukunft zusammen?\"\n",
        "Beispiel: Die Temperatur heute hängt oft mit der Temperatur gestern zusammen.\n",
        "Räumliche Autokorrelation:\n",
        "→ Fragt: \"Wie hängt der Wert an einem Ort mit Werten an anderen Orten zusammen?\"\n",
        "\n",
        "**Achtung!!**\n",
        "Verwende den bivariaten Moran’s I, wenn du den räumlichen Zusammenhang zwischen zwei Variablen in geografischen Daten untersuchen möchtest. Dieser berücksichtigt, wie benachbarte Gebiete miteinander korrelieren und wie sich räumliche Muster bilden.\n",
        "Beispiel: Du analysierst, ob hohe Wasserhöhen in einer Region mit hohen Wasserhöhen in den benachbarten Regionen zusammenhängen und ob der Luftdruck in benachbarten Regionen ebenfalls ähnliche Werte aufweist.\n",
        "Verwende den Pearson’s R, wenn du den linearen Zusammenhang zwischen zwei Variablen messen möchtest, ohne die räumliche Abhängigkeit zu berücksichtigen. Dies ist besonders nützlich, wenn du den direkten Zusammenhang zwischen zwei Variablen untersuchen möchtest, ohne auf ihre geografische Lage zu achten.\n",
        "Beispiel: Du möchtest wissen, ob es einen linearen Zusammenhang zwischen der Wasserhöhe und dem Luftdruck gibt, unabhängig von den geografischen Standorten der Daten.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "#from splot.esda import moran_bv_plot\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from esda.moran import Moran_BV\n",
        "from libpysal.weights import KNN\n",
        "from shapely.geometry import Point\n",
        "from tqdm import tqdm  # Fortschrittsbalken für test_k\n",
        "\n",
        "\n",
        "def calculating_morans_I(ds, var1: str, var2: str, time: str, k=3, plot=False, test_k=False):\n",
        "    # 1. Zeitschritt auswählen\n",
        "    ds = ds.sel(time=time)\n",
        "\n",
        "    # 2. Koordinaten extrahieren\n",
        "    lat, lon = np.meshgrid(ds.latitude.values, ds.longitude.values, indexing='ij')\n",
        "    coords = np.column_stack([lat.ravel(), lon.ravel()])\n",
        "    gdf = gpd.GeoDataFrame({\n",
        "        var1: ds[var1].values.ravel(),\n",
        "        var2: ds[var2].values.ravel()\n",
        "    }, geometry=[Point(xy) for xy in coords])\n",
        "\n",
        "    # 3. NaNs entfernen\n",
        "    gdf = gdf.dropna(subset=[var1, var2]).reset_index(drop=True)\n",
        "\n",
        "    # 4. KNN Spatial Weights\n",
        "    if len(gdf) <= k:\n",
        "        raise ValueError(f\"k={k} is too large for the dataset size of {len(gdf)}\")\n",
        "    w = KNN.from_dataframe(gdf, k=k)\n",
        "    w.transform = \"r\"\n",
        "\n",
        "    # 5. Moran's I berechnen\n",
        "    x = gdf[var1]\n",
        "    y = gdf[var2]\n",
        "    moran_bv = Moran_BV(x, y, w)\n",
        "\n",
        "    # 6. Optional: K-Test\n",
        "    if test_k:\n",
        "        list_k = np.arange(3, min(50, len(gdf)), 3)\n",
        "        results = []\n",
        "\n",
        "        for k_ in tqdm(list_k, desc=\"Testing k values\"):\n",
        "            if len(gdf) <= k_:\n",
        "                continue\n",
        "            try:\n",
        "                w_ = KNN.from_dataframe(gdf, k=k_)\n",
        "                w_.transform = \"r\"\n",
        "                m_bv = Moran_BV(x, y, w_)\n",
        "                results.append({\"k\": k_, \"Moran's I\": m_bv.I, \"p-value\": m_bv.p_sim})\n",
        "            except Exception as e:\n",
        "                print(f\"Fehler bei k={k_}: {e}\")\n",
        "\n",
        "        df_results = pd.DataFrame(results)\n",
        "        print(df_results)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 7. Optional: Plot\n",
        "    if plot:\n",
        "        print(f\"Bivariate Moran’s I: {moran_bv.I:.4f}\")\n",
        "        print(f\"P-Wert (Monte Carlo): {moran_bv.p_sim:.4f}\")\n",
        "\n",
        "        z_x = (x - x.mean()) / x.std()\n",
        "        z_y = (y - y.mean()) / y.std()\n",
        "        wz_y = w.sparse @ z_y\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(z_x, wz_y, alpha=0.5)\n",
        "        plt.axhline(0, color='red', linestyle='--')\n",
        "        plt.axvline(0, color='red', linestyle='--')\n",
        "        plt.title(\"Bivariate Moran's I Scatter Plot\")\n",
        "        plt.xlabel(\"Variable X (Standardized)\")\n",
        "        plt.ylabel(\"Spatial Lag of Y (Standardized)\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return moran_bv.I, moran_bv.p_sim\n",
        "\n",
        "\n",
        "# Usage\n",
        "time = \"2024-10-20 19:00\"\n",
        "var1 = \"sla\"\n",
        "var2 = \"pressure_msl\"\n",
        "\n",
        "morans_I, morans_p = calculating_morans_I(ds_ocean_weather_interp, var1=var1, var2=var2, time=time, k=8, plot=False, test_k=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make list of times between 2024-01-02 18:00 and 2024-01-05 05:00\n",
        "start = df_ocean[\"time\"].min()\n",
        "end = df_ocean[\"time\"].max()\n",
        "\n",
        "start = \"2023-10-01 00:00\"\n",
        "end = \"2023-11-01 00:00\"\n",
        "timepoints = pd.date_range(start=start, end=end, freq=\"1h\")\n",
        "\n",
        "\n",
        "\n",
        "var1 = \"sla\"\n",
        "var2 = \"wind_speed_10m\"\n",
        "\n",
        "list_morans_I = []\n",
        "list_morans_p = []\n",
        "for time in timepoints:\n",
        "    #print(f\"Calculating Moran's I for time: {time}\")\n",
        "    # Calculate Moran's I for each timepoint\n",
        "    morans_I, morans_p = calculating_morans_I(ds_ocean_weather_interp, var1=var1, var2=var2, time=time, k=5, plot=False)\n",
        "    list_morans_I.append(morans_I)\n",
        "    list_morans_p.append(morans_p)\n",
        "\n",
        "array_morans_I = np.array(list_morans_I)\n",
        "array_morans_p = np.array(list_morans_p)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(timepoints, array_morans_I, label=\"Moran's I\", color=\"blue\")\n",
        "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Moran's I over Time\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Moran's I\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ziel: Berechnet die Pearson-Korrelation zwischen zwei Xarray-Daten (x und y), wobei dim='time' spezifiziert, dass die Korrelation entlang der Zeitdimension durchgeführt wird.\n",
        "\n",
        "xr.corr: Dies ist eine Funktion von Xarray, die den Pearson-Korrelationskoeffizienten zwischen zwei Variablen berechnet, indem sie ihre Werte entlang einer bestimmten Dimension (in diesem Fall time) vergleicht.\n",
        "\n",
        "x und y sind die beiden Xarray-Datenarrays (z. B. Zeitserien von sla und pressure_msl).\n",
        "dim='time' bedeutet, dass die Korrelation für jede räumliche Position über die Zeit hinweg berechnet wird. Zum Beispiel, wenn du Zeitserien für verschiedene geografische Punkte hast, wird der Korrelationswert für jeden Punkt berechnet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cartopy.crs as ccrs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select subset of data from start_time to end_time\n",
        "# start_time = \"2023-01-19\"\n",
        "# end_time = \"2023-10-21\"\n",
        "\n",
        "# Select all times in df_ocean where df_insitu[slev] larger then 1.0\n",
        "df_stormsurge = df_insitu[df_insitu[\"slev\"] > 1.0]\n",
        "# Select the timepoints from df_stormsurge\n",
        "timepoints = df_stormsurge[\"time\"].values\n",
        "\n",
        "ds_ocean_weather_interp_sub = ds_ocean_weather_interp.sel(time=timepoints)\n",
        "\n",
        "\n",
        "#time_string = f\"{start_time.strftime('%Y-%m-%d')} to {end_time.strftime('%Y-%m-%d')}\"\n",
        "time_string = f'Correlation of all timpoints if sturm surges'\n",
        "\n",
        "#ds_ocean_weather_interp_sub = ds_ocean_weather_interp.sel(time=slice(start_time, end_time))\n",
        "\n",
        "def pearson_r(x, y):\n",
        "    return xr.corr(x, y, dim='time')\n",
        "\n",
        "def plot_correlation(x, y, title=\"Correlation Map (Contour)\"):\n",
        "    \"\"\"\n",
        "    Plots the correlation map as a contour plot using Cartopy.\n",
        "    \n",
        "    Parameters:\n",
        "        x, y (xarray.DataArray): Variables to compute correlation from.\n",
        "        title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    correlation_map = pearson_r(x, y)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "\n",
        "    # Konturplot\n",
        "    correlation_map.plot.contourf(\n",
        "        ax=ax,\n",
        "        transform=ccrs.PlateCarree(),\n",
        "        cmap='coolwarm', # \n",
        "        levels=50,  # Optional: Anzahl der Konturlinien\n",
        "        vmin=-1,\n",
        "        vmax=1,\n",
        "        cbar_kwargs={'label': 'Pearson Correlation Coefficient','orientation': 'horizontal', 'shrink': 0.8, 'pad': 0.05},\n",
        "\n",
        "    )\n",
        "\n",
        "    ax.coastlines()\n",
        "    ax.add_feature(cfeature.BORDERS)\n",
        "    # colors to land\n",
        "    ax.add_feature(cfeature.LAND, facecolor='lightgrey', alpha=0.9)\n",
        "    # add lat and lon gridlines\n",
        "    ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
        "    ax.set_title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Ignore RuntimeWarning\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "plot_correlation(ds_ocean_weather_interp_sub['sla'], ds_ocean_weather_interp_sub['pressure_msl'], title=f\"Correlation between SLA and Pressure from {time_string}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_correlation_with_currents(x, y, u, v, title=\"Correlation Map (Contour + Currents)\"):\n",
        "    \"\"\"\n",
        "    Plots the correlation map as a contour plot with ocean current arrows using Cartopy.\n",
        "\n",
        "    Parameters:\n",
        "        x, y (xarray.DataArray): Variables to compute correlation from.\n",
        "        u, v (xarray.DataArray): Zonal (uo) and meridional (vo) current components.\n",
        "        title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    correlation_map = pearson_r(x, y)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "\n",
        "    # Konturplot\n",
        "    correlation_map.plot.contourf(\n",
        "        ax=ax,\n",
        "        transform=ccrs.PlateCarree(),\n",
        "        cmap='coolwarm',\n",
        "        levels=21,\n",
        "        vmin=-1,\n",
        "        vmax=1,\n",
        "        cbar_kwargs={'label': 'Pearson Correlation Coefficient',\n",
        "                     'orientation': 'horizontal', 'shrink': 0.8, 'pad': 0.05},\n",
        "    )\n",
        "\n",
        "    # Subsampling für bessere Übersicht\n",
        "    step = 1\n",
        "    u_sub = u.isel(latitude=slice(None, None, step), longitude=slice(None, None, step))\n",
        "    v_sub = v.isel(latitude=slice(None, None, step), longitude=slice(None, None, step))\n",
        "\n",
        "    # Gitterkoordinaten extrahieren\n",
        "    lat_sub = u_sub.latitude.values\n",
        "    lon_sub = u_sub.longitude.values\n",
        "    lon2d, lat2d = np.meshgrid(lon_sub, lat_sub)\n",
        "\n",
        "    # Quiver-Pfeile plotten (auf 2D-Arrays achten!)\n",
        "    ax.quiver(\n",
        "        lon2d,\n",
        "        lat2d,\n",
        "        u_sub.values,\n",
        "        v_sub.values,\n",
        "        transform=ccrs.PlateCarree(),\n",
        "        scale=10,  # Anpassen nach Daten\n",
        "        width=0.002,\n",
        "        color='black'\n",
        "    )\n",
        "\n",
        "    # Zusätzliche Kartenfeatures\n",
        "    ax.coastlines()\n",
        "    ax.add_feature(cfeature.BORDERS)\n",
        "    ax.add_feature(cfeature.LAND, facecolor='lightgrey', alpha=0.9)\n",
        "    ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
        "    ax.set_title(title)\n",
        "    plt.show()\n",
        "\n",
        "u_mean = ds_ocean_weather_interp_sub['uo'].mean(dim='time')\n",
        "v_mean = ds_ocean_weather_interp_sub['vo'].mean(dim='time')\n",
        "\n",
        "\n",
        "plot_correlation_with_currents(\n",
        "    ds_ocean_weather_interp_sub['sla'],\n",
        "    ds_ocean_weather_interp_sub['pressure_msl'],\n",
        "    u_mean,\n",
        "    v_mean,\n",
        "    title=f\"Correlation between SLA and Pressure with Currents from {time_string}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "\n",
        "def select_nearest_valid_point(ds, variable_name, target_lat, target_lon):\n",
        "    \"\"\"\n",
        "    Wähle den nächstgelegenen gültigen Punkt (nicht-NaN) im Dataset für eine bestimmte Variable.\n",
        "    \n",
        "    Parameters:\n",
        "        ds (xr.Dataset): Das Eingabe-Dataset mit latitude, longitude, und time-Dimensionen.\n",
        "        variable_name (str): Name der Variable zur Prüfung auf Gültigkeit (z.B. 'wind_speed_10m').\n",
        "        target_lat (float): Ziel-Breitengrad.\n",
        "        target_lon (float): Ziel-Längengrad.\n",
        "\n",
        "    Returns:\n",
        "        xr.Dataset: Subset des ursprünglichen Datasets an der nächsten gültigen Position.\n",
        "        float: Breitengrad der gültigen Position.\n",
        "        float: Längengrad der gültigen Position.\n",
        "    \"\"\"\n",
        "    # Maske gültiger Punkte entlang der Zeitachse\n",
        "    valid_mask = ds[variable_name].notnull().any(dim='time')\n",
        "\n",
        "    # 2D-Gitter der Koordinaten\n",
        "    lat2d, lon2d = np.meshgrid(ds.latitude.values, ds.longitude.values, indexing='ij')\n",
        "\n",
        "    # Nur gültige Koordinaten extrahieren\n",
        "    valid_lat_points = lat2d[valid_mask.values]\n",
        "    valid_lon_points = lon2d[valid_mask.values]\n",
        "\n",
        "    # Distanzberechnung (euklidisch)\n",
        "    distances = np.sqrt((valid_lat_points - target_lat)**2 + (valid_lon_points - target_lon)**2)\n",
        "\n",
        "    # Index des nächsten gültigen Punkts\n",
        "    min_idx = np.argmin(distances)\n",
        "    nearest_lat = valid_lat_points[min_idx]\n",
        "    nearest_lon = valid_lon_points[min_idx]\n",
        "\n",
        "    print(f\"Nächstgelegener gültiger Punkt: lat={nearest_lat:.6f}, lon={nearest_lon:.6f}\")\n",
        "    \n",
        "    return ds.sel(latitude=nearest_lat, longitude=nearest_lon), nearest_lat, nearest_lon\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xarray as xr\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# === 1. In-situ Daten laden ===\n",
        "# def load_insitu_data(filepath):\n",
        "#     ds = xr.open_dataset(filepath)\n",
        "#     df = ds.to_dataframe().reset_index()\n",
        "#     return df.rename(columns={\n",
        "#         \"TIME\": \"time\",\n",
        "#         \"SLEV\": \"slev\",\n",
        "#         \"LATITUDE\": \"latitude\",\n",
        "#         \"LONGITUDE\": \"longitude\"\n",
        "#     })\n",
        "\n",
        "df_insitu = load_insitu_data()\n",
        "df_insitu = flensburg_data_processing(df_insitu)\n",
        "df_insitu = interpolate_missing_times(df_insitu)\n",
        "\n",
        "insitu_location = (df_insitu.latitude.iloc[0], df_insitu.longitude.iloc[0])\n",
        "\n",
        "# Get the nearest valid point in xarray to Flensburg\n",
        "target_lat = 54.5\n",
        "target_lon = 10.0\n",
        "flensburg_ds, nearest_lat, nearest_lon = select_nearest_valid_point(ds_ocean_weather_interp_sub, 'wind_speed_10m', target_lat, target_lon)\n",
        "\n",
        "\n",
        "\n",
        "# === 2. Wetterdaten vorbereiten ===\n",
        "# Stelle sicher, dass flensburg_ds vorher korrekt definiert ist\n",
        "wetter_df = flensburg_ds.to_dataframe().reset_index().set_index(\"time\")\n",
        "wetter_location = (wetter_df.latitude.iloc[0], wetter_df.longitude.iloc[0])\n",
        "\n",
        "\n",
        "# === 3. Karte mit Basemap zeichnen ===\n",
        "def plot_locations(insitu_loc, weather_loc):\n",
        "    center_lat = (insitu_loc[0] + weather_loc[0]) / 2\n",
        "    center_lon = (insitu_loc[1] + weather_loc[1]) / 2\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    m = Basemap(projection='merc',\n",
        "                llcrnrlat=center_lat - 2, urcrnrlat=center_lat + 2,\n",
        "                llcrnrlon=center_lon - 2, urcrnrlon=center_lon + 2,\n",
        "                resolution='i')\n",
        "\n",
        "    m.drawcoastlines()\n",
        "    m.drawcountries()\n",
        "    m.drawmapboundary(fill_color='white')\n",
        "    m.fillcontinents(color='lightgray', lake_color='white')\n",
        "    m.drawparallels(np.arange(0., 90., 0.5), labels=[1,0,0,0])\n",
        "    m.drawmeridians(np.arange(0., 180., 0.5), labels=[0,0,0,1])\n",
        "\n",
        "    x_insitu, y_insitu = m(insitu_loc[1], insitu_loc[0])\n",
        "    x_weather, y_weather = m(weather_loc[1], weather_loc[0])\n",
        "\n",
        "    m.plot(x_insitu, y_insitu, 'bo', markersize=8, label='In-situ Location')\n",
        "    m.plot(x_weather, y_weather, 'ro', markersize=8, label='Weather Location')\n",
        "\n",
        "    plt.text(x_insitu+10000, y_insitu+10000, 'Flensburg In-Situ', \n",
        "             #fontsize=12, \n",
        "             color='black')\n",
        "    plt.text(x_weather+10000, y_weather+10000, 'Closest Point', \n",
        "             #fontsize=12, \n",
        "             color='red')\n",
        "\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title('In-situ vs. Wetterpunkt')\n",
        "    plt.show()\n",
        "\n",
        "plot_locations(insitu_location, wetter_location)\n",
        "\n",
        "# === 4. Zeitliche Synchronisierung ===\n",
        "df_insitu = df_insitu.set_index(\"time\").resample(\"h\").mean(numeric_only=True)\n",
        "wetter_df = wetter_df.resample(\"h\").mean()\n",
        "\n",
        "# === 5. Daten zusammenführen ===\n",
        "merged_df = pd.merge(df_insitu, wetter_df, left_index=True, right_index=True, how=\"inner\")\n",
        "display(merged_df.head(2))\n",
        "display(merged_df.tail(2))\n",
        "\n",
        "# === 6. Explorative Analyse ===\n",
        "features = ['sla', 'slev', 'wind_speed_10m', 'surface_pressure', 'precipitation', 'wind_direction_10m', 'vo']\n",
        "#features = merged_df.columns.tolist()\n",
        "\n",
        "\n",
        "sns.pairplot(merged_df[features], diag_kind='kde')\n",
        "plt.show()\n",
        "\n",
        "# === 7. Korrelationen anzeigen ===\n",
        "correlation = merged_df.corr(numeric_only=True)\n",
        "print(\"Korrelationsmatrix:\")\n",
        "display(correlation[\"slev\"].sort_values(ascending=False))\n",
        "\n",
        "# 10 grö0te Korrelationen\n",
        "features = correlation[\"slev\"].sort_values(ascending=False).nlargest(10).index.tolist()\n",
        "features += correlation[\"slev\"].sort_values(ascending=False).nsmallest(10).index.tolist()\n",
        "print(features)\n",
        "\n",
        "# === 8. Zeitreihenvisualisierung ===\n",
        "merged_df[features].plot(\n",
        "    subplots=True, figsize=(10, 17), title=f\"Zeitreihen from {time_string}\")\n",
        "plt.tight_layout()\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel(\"Time\")\n",
        "plt.show()\n",
        "\n",
        "# === 9. Lineare Regression ===\n",
        "def run_regression(df, target_col, feature_cols):\n",
        "    X = df[feature_cols]\n",
        "    y = df[target_col]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    #X_scaled = X\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_scaled, y)\n",
        "\n",
        "    print(\"\\nRegressionskoeffizienten:\")\n",
        "    for feat, coef in zip(feature_cols, model.coef_):\n",
        "        print(f\"{feat}: {coef:.4f}\")\n",
        "    print(f\"R² Score: {model.score(X_scaled, y):.4f}\")\n",
        "\n",
        "run_regression(merged_df, \"slev\", features[1:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_merged_sub_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_scatter(ds_merged, x_col, y_col, c, title, xlabel, ylabel, dim=['latitude', 'longitude']):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(ds_merged[x_col].mean(dim=dim), ds_merged[y_col].mean(dim=dim), c=ds_merged[c].mean(dim=dim), cmap='viridis', alpha=0.5)\n",
        "    plt.colorbar(label=c)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "\n",
        "plot_scatter(ds_merged=ds_merged_sub, \n",
        "             x_col='wind_speed_10m', \n",
        "             y_col='sla',\n",
        "             c='sla',\n",
        "            title='SLA vs Pressure MSL',\n",
        "            xlabel='SLA [m]',\n",
        "            ylabel='sla',\n",
        "            )\n",
        "\n",
        "\n",
        "# col1 = ds_merged['sla'].mean(dim=['latitude', 'longitude'])\n",
        "# col2 = ds_merged['wind_speed_10m'].mean(dim=['latitude', 'longitude'])\n",
        "\n",
        "# plt.scatter(col1, col2, alpha=0.5, marker='o', cmap='viridis', c=ds_merged['wind_direction_10m'].mean(dim=['latitude', 'longitude']))\n",
        "# plt.xlabel('mean SLA [m]')\n",
        "# plt.ylabel('mean Wind Speed [m/s]')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "\n",
        "def plot_contour(ds_merged, x_col, y_col, c, title, xlabel, ylabel, bins=500, dim=['latitude', 'longitude']):\n",
        "    \"\"\"\n",
        "    Erzeugt einen Contourplot von Mittelwerten über angegebene Dimensionen (z. B. lat/lon).\n",
        "    \"\"\"\n",
        "    # Mittelwerte über Raumdimensionen\n",
        "    x = ds_merged[x_col].mean(dim=dim).values.flatten()\n",
        "    y = ds_merged[y_col].mean(dim=dim).values.flatten()\n",
        "    z = ds_merged[c].mean(dim=dim).values.flatten()\n",
        "\n",
        "    # Entferne NaNs\n",
        "    mask = ~np.isnan(x) & ~np.isnan(y) & ~np.isnan(z)\n",
        "    x, y, z = x[mask], y[mask], z[mask]\n",
        "\n",
        "    # Erzeuge ein 2D-Gitter durch Histogramm-Binning\n",
        "    xi = np.linspace(np.min(x), np.max(x), bins)\n",
        "    yi = np.linspace(np.min(y), np.max(y), bins)\n",
        "    Xi, Yi = np.meshgrid(xi, yi)\n",
        "\n",
        "    # Interpolation der Z-Werte auf das Gitter\n",
        "    from scipy.interpolate import griddata\n",
        "    Zi = griddata((x, y), z, (Xi, Yi), method='linear')\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    contour = plt.contourf(Xi, Yi, Zi, levels=50, cmap='viridis')\n",
        "    cbar = plt.colorbar(contour)\n",
        "    cbar.set_label(c)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_contour(ds_merged=ds_merged, \n",
        "             x_col='wind_speed_10m', \n",
        "             y_col='wind_direction_10m',\n",
        "             c='sla',\n",
        "             title='Contourplot: SLA vs Pressure MSL',\n",
        "             xlabel='SLA [m]',\n",
        "             ylabel='Pressure MSL [hPa]',\n",
        "             bins=100,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_interp = ds_weather.interpolate_na(dim=\"latitude\", method=\"linear\")\n",
        "ds_interp = ds_interp.interpolate_na(dim=\"longitude\", method=\"linear\")\n",
        "ds_interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Altes Gitter\n",
        "old_lats = ds_interp.latitude\n",
        "old_lons = ds_interp.longitude\n",
        "\n",
        "# Neues feineres Gitter erzeugen (z. B. 0.25° statt 1.0° Auflösung)\n",
        "new_lats = np.arange(old_lats.min(), old_lats.max(), 0.25)\n",
        "new_lons = np.arange(old_lons.min(), old_lons.max(), 0.25)\n",
        "\n",
        "# Interpolation\n",
        "ds_interp = ds_interp.interp(latitude=new_lats, longitude=new_lons, method='linear')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Beispiel: Temperatur auswählen\n",
        "temperature = ds_interp['pressure_msl'].sel(time='2023-10-18T12:00:00')\n",
        "\n",
        "# Plot\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = plt.axes(projection=ccrs.PlateCarree())  # oder z. B. ccrs.Mercator()\n",
        "\n",
        "temperature.plot(ax=ax, transform=ccrs.PlateCarree(), cmap='coolwarm', cbar_kwargs={'label': '°C'})\n",
        "\n",
        "# Extras: Küstenlinien etc.\n",
        "ax.coastlines()\n",
        "ax.add_feature(cfeature.BORDERS)\n",
        "ax.set_title(\"Temperaturkarte\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temperature = ds_interp['pressure_msl'].sel(time='2023-10-18T12:00:00')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "ax.coastlines()\n",
        "ax.add_feature(cfeature.BORDERS)\n",
        "\n",
        "# Konturlinien\n",
        "cs = ax.contourf(temperature.longitude, temperature.latitude, temperature, \n",
        "                 levels=20, cmap='coolwarm',  # <- Korrektur hier\n",
        "                 linewidths=1, transform=ccrs.PlateCarree())\n",
        "\n",
        "\n",
        "ax.set_title(\"Konturlinien des Luftdrucks\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "u = ds_interp['u10'].sel(time='2023-10-18T12:00:00')  # Ost-Komponente\n",
        "v = ds_interp['v10'].sel(time='2023-10-18T12:00:00')  # Nord-Komponente\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "ax.coastlines()\n",
        "ax.add_feature(cfeature.BORDERS)\n",
        "\n",
        "# Vektorpfeile (Wind)\n",
        "q = ax.quiver(u.lon[::5], u.lat[::5], u[::5, ::5], v[::5, ::5], transform=ccrs.PlateCarree(), scale=700)\n",
        "\n",
        "ax.set_title(\"Windvektoren\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "ax.coastlines()\n",
        "\n",
        "ax.streamplot(u.lon, u.lat, u.values, v.values, transform=ccrs.PlateCarree(), color='blue', density=1.5)\n",
        "ax.set_title(\"Stromlinien der Windrichtung\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp = ds_interp['temperature_2m'].mean(dim=['latitude', 'longitude'])\n",
        "humidity = ds_interp['relative_humidity_2m'].mean(dim=['latitude', 'longitude'])\n",
        "\n",
        "plt.scatter(temp, humidity, alpha=0.5)\n",
        "plt.xlabel('Temperatur [°C]')\n",
        "plt.ylabel('relative Luftfeuchtigkeit [%]')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "# Beispielzeitpunkt auswählen\n",
        "time_sel = '2023-10-20T12:00'\n",
        "\n",
        "# Daten selektieren\n",
        "temp = ds_interp['relative_humidity_2m'].sel(time=time_sel)\n",
        "wind_speed = ds_interp['wind_speed_10m'].sel(time=time_sel)\n",
        "wind_dir = ds_interp['wind_direction_10m'].sel(time=time_sel)\n",
        "\n",
        "# Windrichtung und -geschwindigkeit → u, v-Komponenten umrechnen\n",
        "wind_u = wind_speed * -np.sin(np.deg2rad(wind_dir))\n",
        "wind_v = wind_speed * -np.cos(np.deg2rad(wind_dir))\n",
        "\n",
        "# Plot erstellen\n",
        "fig, ax = plt.subplots(figsize=(12, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "\n",
        "# Temperaturkarte (colormap)\n",
        "temp.plot(ax=ax, transform=ccrs.PlateCarree(), cmap='coolwarm', cbar_kwargs={'label': 'Temperatur [°C]'})\n",
        "\n",
        "# Küstenlinie, Ländergrenzen etc.\n",
        "ax.coastlines()\n",
        "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "ax.add_feature(cfeature.LAND, facecolor='lightgray', alpha=0.3)\n",
        "\n",
        "# Windvektoren\n",
        "# Downsamplen für bessere Übersicht (z. B. jeden 3. Punkt)\n",
        "step = 2\n",
        "lat = ds_interp.latitude[::step]\n",
        "lon = ds_interp.longitude[::step]\n",
        "u = wind_u[::step, ::step]\n",
        "v = wind_v[::step, ::step]\n",
        "\n",
        "ax.quiver(lon, lat, u, v, transform=ccrs.PlateCarree(), color='black', scale=700)\n",
        "\n",
        "# Titel\n",
        "ax.set_title(f\"Temperatur und Wind am {time_sel}\", \n",
        "             #fontsize=14\n",
        "             )\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def process_coord(df: pd.DataFrame, coord: tuple) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes a subset of the DataFrame for a given coordinate.\n",
        "    \"\"\"\n",
        "    df_sub = df[df['position'] == coord].copy()\n",
        "    df_sub.drop(columns=['latitude', 'longitude', 'position'], inplace=True)\n",
        "\n",
        "    value_columns = df_sub.columns.difference(['time'])\n",
        "    df_sub.rename(\n",
        "        columns={col: f\"{col}_{coord}\" for col in value_columns},\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    return df_sub\n",
        "\n",
        "def convert_df_joblib(df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converts the DataFrame by pivoting values for unique coordinates\n",
        "    using parallel processing via joblib.\n",
        "    \"\"\"\n",
        "    df['position'] = list(zip(df['latitude'], df['longitude']))\n",
        "    unique_coords = df['position'].unique()\n",
        "    print(f\"Number of unique coordinates: {len(unique_coords)}\")\n",
        "\n",
        "    df_merged = pd.DataFrame({'time': df['time'].unique()})\n",
        "\n",
        "    # Parallel processing\n",
        "    results = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(process_coord)(df, coord) for coord in tqdm(unique_coords)\n",
        "    )\n",
        "\n",
        "    # Merge all partial DataFrames\n",
        "    for df_sub in results:\n",
        "        df_merged = df_merged.merge(df_sub, on='time', how='left')\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "# Usage\n",
        "df_ocean_converted = convert_df_joblib(df_ocean)\n",
        "df_weather_converted = convert_df_joblib(df_weather)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge df_ocean and df_weather and df_insitu\n",
        "df_merged = df_ocean_converted.merge(df_weather_converted, on='time', how='inner')\n",
        "df_merged = df_merged.merge(df_insitu, on='time', how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "corr = df_merged.corr()\n",
        "# mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "# sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", annot=False, fmt=\".2f\", square=True, cbar_kws={\"shrink\": .8})\n",
        "# plt.title(\"Correlation Matrix\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the highest correlation between the columns and slev\n",
        "corr_slev = corr[\"slev\"].nlargest(100)\n",
        "\n",
        "\n",
        "# Display the correlation values in a bar plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=corr_slev.index, y=corr_slev.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Correlation with slev\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_slev.index "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the highest correlation between the columns and slev\n",
        "corr_slev = corr[\"slev\"].nsmallest(100)\n",
        "\n",
        "\n",
        "# Display the correlation values in a bar plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=corr_slev.index, y=corr_slev.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Correlation with slev\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_index_name(corr_slev: pd.Series, name: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the first index name that contains the specified substring.\n",
        "    \"\"\"\n",
        "    for col in corr_slev.index:\n",
        "        if name in col:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "# Get the index name of the column that contains 'wind'\n",
        "sla_col = get_index_name(corr_slev, \"sla\")\n",
        "wo_col = get_index_name(corr_slev, \"wo\")\n",
        "pressure_msl_col = get_index_name(corr_slev, \"pressure_msl\")\n",
        "surface_pressure_col = get_index_name(corr_slev, \"surface_pressure\")\n",
        "print(f\"Wind column: {sla_col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def scale_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scale the DataFrame using MinMaxScaler, excluding the 'time' column.\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    \n",
        "    # Save time column and drop it from the data to be scaled\n",
        "    time = df[\"time\"]\n",
        "    data_to_scale = df.drop(columns=[\"time\"])\n",
        "\n",
        "    # Fit and transform the data (excluding 'time')\n",
        "    scaled_values = scaler.fit_transform(data_to_scale)\n",
        "\n",
        "    # Create scaled DataFrame\n",
        "    df_scaled = pd.DataFrame(scaled_values, columns=data_to_scale.columns)\n",
        "    df_scaled[\"time\"] = time\n",
        "\n",
        "    return df_scaled\n",
        "\n",
        "# Beispiel-Nutzung\n",
        "df_scaled = scale_df(df_merged)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_position_of_column(df: pd.DataFrame, col_name: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Get the position (latitude, longitude) of a specific column in the DataFrame.\n",
        "    \"\"\"\n",
        "    # Split the column name to extract latitude and longitude\n",
        "    parts = col_name.split(\"_\")[-1]\n",
        "    parts = eval(parts)\n",
        "    lat = float(parts[-2])\n",
        "    lon = float(parts[-1])\n",
        "    return lat, lon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pressure_msl_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot slev\n",
        "\n",
        "# normalize the like MinMaxScaler\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot the water level\n",
        "ax.plot(df_scaled['time'], df_scaled['slev'], label='Wasserstand', color='blue')\n",
        "\n",
        "# plot the sla_col \n",
        "#ax.plot(df_scaled['time'], df_scaled[sla_col], label=sla_col, color='red')\n",
        "\n",
        "ax.plot(df_scaled['time'], df_scaled[wo_col], label=wo_col, color='green')\n",
        "\n",
        "#ax.plot(df_scaled['time'], df_scaled[pressure_msl_col], label=pressure_msl_col, color='orange')\n",
        "\n",
        "\n",
        "# plot the position of the column on map\n",
        "lat, lon = get_position_of_column(df_scaled, wo_col)\n",
        "print(f\"Position of {wo_col}: {lat}, {lon}\")\n",
        "# Create a Basemap\n",
        "fig_map, ax_map = plt.subplots(figsize=(12, 10))\n",
        "m = Basemap(\n",
        "    projection=\"cyl\",\n",
        "    resolution=\"i\",\n",
        "    llcrnrlon=lon_grid.min(),\n",
        "    urcrnrlon=lon_grid.max(),\n",
        "    llcrnrlat=lat_grid.min(),\n",
        "    urcrnrlat=lat_grid.max(),\n",
        "    ax=ax_map,\n",
        ")\n",
        "# Draw map features\n",
        "m.drawcoastlines()\n",
        "m.drawcountries()\n",
        "m.fillcontinents(color=\"0.8\")\n",
        "m.drawstates()\n",
        "m.drawmapboundary(fill_color=\"aqua\")\n",
        "m.fillcontinents(color=\"coral\", lake_color=\"aqua\", alpha=0.2)\n",
        "# Scatterplot for ocean data\n",
        "x, y = m(lon, lat)\n",
        "m.scatter(x, y, color=\"blue\", label=\"Ocean Data\", zorder=5)\n",
        "# Add a title and legend\n",
        "plt.title(f\"Position of {pressure_msl_col} on map\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "geomar-deeplearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "040eea0881a04d02b7faf2c3effca4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12379b91f43d477fa27170daddeb9369": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1383b02a5f4049a080c3ca2c325c6687": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26543bcb1f034139ba024498bdd39439",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b84f47ad66d44767a8a978fd8a585aef",
            "value": 2
          }
        },
        "158843476d7541beb3067ca7127de1a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24f5d3de61ae4482b3c8b8e0a48351aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26543bcb1f034139ba024498bdd39439": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5357c36f444952bd174a0619bda09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849c76b486d5441b89ce28d3cd1411a1",
            "placeholder": "​",
            "style": "IPY_MODEL_706206b9195947ef9ea01b859a75d8fb",
            "value": " 2/2 [00:00&lt;00:00,  5.50it/s]"
          }
        },
        "506b51b02f264362a837c7b63b2fa16e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "517d5f66cd154580adc0d63ced53717a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f6c7fd47c6f408db604aa37bc3bb611": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f5d3de61ae4482b3c8b8e0a48351aa",
            "max": 330,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_040eea0881a04d02b7faf2c3effca4d9",
            "value": 330
          }
        },
        "634e039630b242f9b35a47045a1b8d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd3fad87eec54068822b0721e58e9796",
              "IPY_MODEL_5f6c7fd47c6f408db604aa37bc3bb611",
              "IPY_MODEL_efbe98b509c444509d33745aa79036c0"
            ],
            "layout": "IPY_MODEL_703f229377d14f6e999cb2ab8e6e3de6"
          }
        },
        "703f229377d14f6e999cb2ab8e6e3de6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "706206b9195947ef9ea01b859a75d8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "849c76b486d5441b89ce28d3cd1411a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94097d8ac96d4258af721da99d479a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc90ebde1df94b8e921938d6146e2617",
              "IPY_MODEL_1383b02a5f4049a080c3ca2c325c6687",
              "IPY_MODEL_2f5357c36f444952bd174a0619bda09b"
            ],
            "layout": "IPY_MODEL_506b51b02f264362a837c7b63b2fa16e"
          }
        },
        "b84f47ad66d44767a8a978fd8a585aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba21969bab124b4e89111d677f64de17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c52f4a116ac0475792786df43391da31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd3fad87eec54068822b0721e58e9796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12379b91f43d477fa27170daddeb9369",
            "placeholder": "​",
            "style": "IPY_MODEL_c52f4a116ac0475792786df43391da31",
            "value": "100%"
          }
        },
        "dc90ebde1df94b8e921938d6146e2617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_158843476d7541beb3067ca7127de1a0",
            "placeholder": "​",
            "style": "IPY_MODEL_ba21969bab124b4e89111d677f64de17",
            "value": "100%"
          }
        },
        "efbe98b509c444509d33745aa79036c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_517d5f66cd154580adc0d63ced53717a",
            "placeholder": "​",
            "style": "IPY_MODEL_fa084275366141008eebae1bb628e9cb",
            "value": " 330/330 [00:20&lt;00:00, 18.45it/s]"
          }
        },
        "fa084275366141008eebae1bb628e9cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
